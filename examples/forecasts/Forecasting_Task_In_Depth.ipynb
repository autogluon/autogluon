{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook introduce more advanced techniques in Forecasting tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the quick start tutourial, we will do forecasting related to the COV19 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.forecasting import Forecasting as task\n",
    "from autogluon.forecasting import ForecastingPredictor\n",
    "\n",
    "train_data = task.Dataset(\"https://autogluon.s3-us-west-2.amazonaws.com/datasets/CovidTimeSeries/train.csv\")\n",
    "test_data = task.Dataset(\"https://autogluon.s3-us-west-2.amazonaws.com/datasets/CovidTimeSeries/test.csv\")\n",
    "\n",
    "# uncessary as it's the default metric\n",
    "eval_metric = \"mean_wQuantileLoss\"\n",
    "# uncessary as it's the default searcher\n",
    "searcher_type = \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify hyperparameters and tuning them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do hyperparameter tuning using autogluon, and here the variable \"context_length\" is the one that we would like to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogluon.core as ag\n",
    "from gluonts.distribution.neg_binomial import NegativeBinomialOutput\n",
    "\n",
    "\n",
    "mqcnn_params = {\n",
    "    \"context_length\": ag.Int(75, 100),\n",
    "    \"num_batches_per_epoch\": 10,\n",
    "    \"epochs\": 30\n",
    "}\n",
    "\n",
    "deepar_params = {\n",
    "    \"context_length\": ag.Int(75, 100),\n",
    "    \"num_batches_per_epoch\": 10,\n",
    "    \"distr_output\": NegativeBinomialOutput(),\n",
    "    \"epochs\": 30\n",
    "}\n",
    "\n",
    "sff_params = {\n",
    "    \"context_length\": ag.Int(75, 100),\n",
    "    \"num_batches_per_epoch\": 30,\n",
    "    \"epochs\": 30\n",
    "}\n",
    "\n",
    "predictor = task.fit(train_data=train_data,\n",
    "                     prediction_length=19,\n",
    "                     index_column=\"name\",\n",
    "                     target_column=\"ConfirmedCases\",\n",
    "                     time_column=\"Date\",\n",
    "                     hyperparameter_tune=True,\n",
    "                     quantiles=[0.1, 0.5, 0.9],\n",
    "                     hyperparameters={\"MQCNN\": mqcnn_params,\n",
    "                                      \"DeepAR\": deepar_params,\n",
    "                                      \"SFF\": sff_params\n",
    "                                      },\n",
    "                     search_strategy=searcher_type,\n",
    "                     eval_metric=eval_metric,\n",
    "                     num_trials=3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again demonstrate how to use the trained models to predict on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the performance of each model on test data, we can use the leaderboard() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the predictor will use the best more on the validation set to do the prediction. The prediction results we get is a dictionary whose key is each time series's index and value is the corresponding prediction dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictor.predict(test_data)\n",
    "predictions[\"Afghanistan_\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which model is the best, we can call the get_model_best() method for trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor._trainer.get_model_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides using the best model on validation data, we can also specify which model we want to use for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = predictor._trainer.get_model_names_all()\n",
    "specific_model = predictor._trainer.load_model(model_trained[0])\n",
    "specific_model.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_predictions = predictor.predict(test_data, model=specific_model)\n",
    "specific_predictions[\"Afghanistan_\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
