# Conv-LoRA: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model (ICLR 2024)

Examples showing how to use `Conv-LoRA` for parameter efficient fine-tuning SAM.

## 1. Installation
The installation may take a while since AutoGluon Multimodal has multiple dependencies.
```shell
  conda create --name conv-lora python=3.10
  conda activate conv-lora
  pip install -U pip
  pip install -U setuptools wheel
  pip install autogluon.multimodal
  ```

## 2. Dataset

The datasets will be downloaded in the "datasets" folder:

`python prepare_semantic_segmentation_datasets.py`

## 3. Training

`python run_semantic_segmentation.py --<flag> <value>`

- `task` determines to run the experiments on which task, refers to [Dataset Section](##1-Datasets).
- `seed` determines the random seed.
- `rank` determines the rank of Conv-LoRA. Default is 3.
- `expert_num` determines the used expert number of Conv-LoRA. Default is 8.
- `num_gpus` determines the number of gpu used for training. Default is 1.
- `output_dir` determines the path of output directory. Default is "outputs" folder.
- `ckpt_path` determines the path of model for evaluation. Default is "outputs" folder.

## 4. Evaluation

After running the benchmark, the evaluation results of test set are stored in "{output_dir}/metrics.txt".

You can also run the following command to evaluate a checkpoint:

`python3 run_semantic_segmentation.py --task {dataset_name} --output_dir {output_dir} --ckpt_path {ckpt_path} --eval`


### Citation

```
@article{zhong2024convolution,
  title={Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model},
  author={Zhong, Zihan and Tang, Zhiqiang and He, Tong and Fang, Haoyang and Yuan, Chun},
  journal={arXiv preprint arXiv:2401.17868},
  year={2024}
}
```