{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e18db3",
   "metadata": {},
   "source": [
    "# Custom Metrics\n",
    "\n",
    "In {doc}`model_validation`, we described how to choose a built-in evaluation metric to guide the model selection. This tutorial, we will show how to add your custom metric to AutoGluon.\n",
    "\n",
    "A metric measures model's performance by comparing the difference between true values and predicted values. In the following example, we implement an accuracy metric from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f932760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T21:33:53.224106Z",
     "start_time": "2022-07-12T21:33:53.220899Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / y_true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc724a9",
   "metadata": {},
   "source": [
    "Verify its correctness with toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140ab405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T21:33:54.871816Z",
     "start_time": "2022-07-12T21:33:54.866159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([0, 1, 0, 0])\n",
    "y_pred = np.array([0, 1, 1, 0])\n",
    "\n",
    "my_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4f825",
   "metadata": {},
   "source": [
    "Next we need to wrap our metric to {class}`autogluon.core.metrics.Scorer`, AutoGluon's class for metrics. The easy way to do it is through {func}`autogluon.core.metrics.make_scorer`. It needs to specify four arguments: \n",
    "\n",
    "- the string `name` that will appear in printing\n",
    "- the metric function (`score_func`), which accepts two arguments, `y_true` and `y_pred`, to return a score\n",
    "- the optimal value (`optimum`) when prediction is perfect. It is 1.0 for accuracy, and often 0.0 for a loss.\n",
    "- if a larger returned value is better (`greater_is_better=True`), true for accuracy and false for a loss\n",
    "\n",
    "Note that we need to save our code into a `.py` file so it can be pickled when saving models. Otherwise you will see errors such as `Can't pickle <function...`. We use the `writefile` magic to save the following code into `my_accuracy_ag.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd84a9f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T21:44:35.671505Z",
     "start_time": "2022-07-12T21:44:35.667249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_accuracy_ag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_accuracy_ag.py\n",
    "from autogluon.core.metrics import make_scorer\n",
    "\n",
    "def my_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / y_true.size\n",
    "\n",
    "my_accuracy_ag = make_scorer(\n",
    "    name='accuracy', score_func=my_accuracy,\n",
    "    optimum=1, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba1bd1",
   "metadata": {},
   "source": [
    "To use our metric during training, we need to import it and pass it to `fit` through the `eval_metric` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd452706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T21:35:30.195256Z",
     "start_time": "2022-07-12T21:35:28.522235Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Load the knot theory data\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mli/ag-docs/main/knot_theory/'\n",
    "train_data = TabularDataset(url+'train.csv')\n",
    "test_data = TabularDataset(url+'test.csv')\n",
    "label = 'signature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e16c7049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T21:46:19.361810Z",
     "start_time": "2022-07-12T21:44:40.457263Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220712_214440/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220712_214440/\"\n",
      "AutoGluon Version:  0.5.0\n",
      "Python Version:     3.9.12\n",
      "Operating System:   Linux\n",
      "Train Data Rows:    10000\n",
      "Train Data Columns: 18\n",
      "Label Column: signature\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 13) unique label values:  [-2, 0, 2, -8, 4, -4, -6, 8, 6, 10]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 9 out of 13 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9984\n",
      "Train Data Class Count: 9\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    241954.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.44 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Symmetry_D8']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 14 | ['chern_simons', 'cusp_volume', 'injectivity_radius', 'longitudinal_translation', 'meridinal_translation_imag', ...]\n",
      "\t\t('int', [])   :  3 | ['Unnamed: 0', 'hyperbolic_adjoint_torsion_degree', 'hyperbolic_torsion_degree']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 9 | ['chern_simons', 'cusp_volume', 'injectivity_radius', 'longitudinal_translation', 'meridinal_translation_imag', ...]\n",
      "\t\t('int', [])       : 3 | ['Unnamed: 0', 'hyperbolic_adjoint_torsion_degree', 'hyperbolic_torsion_degree']\n",
      "\t\t('int', ['bool']) : 5 | ['Symmetry_0', 'Symmetry_D3', 'Symmetry_D4', 'Symmetry_D6', 'Symmetry_Z/2 + Z/2']\n",
      "\t0.1s = Fit runtime\n",
      "\t17 features in original data used to generate 17 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 8985, Val Rows: 999\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.2232\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.2132\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9439\t = Validation score   (accuracy)\n",
      "\t8.93s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9459\t = Validation score   (accuracy)\n",
      "\t5.16s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.956\t = Validation score   (accuracy)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9429\t = Validation score   (accuracy)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9499\t = Validation score   (accuracy)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "/home/ubuntu/miniconda3/envs/ag/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "\t0.956\t = Validation score   (accuracy)\n",
      "\t18.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9469\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9399\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.957\t = Validation score   (accuracy)\n",
      "\t6.08s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9389\t = Validation score   (accuracy)\n",
      "\t38.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9499\t = Validation score   (accuracy)\n",
      "\t9.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.965\t = Validation score   (accuracy)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 98.89s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220712_214440/\")\n"
     ]
    }
   ],
   "source": [
    "from my_accuracy_ag import my_accuracy_ag\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=label, eval_metric=my_accuracy_ag).fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e9127",
   "metadata": {},
   "source": [
    "We can also use it to evaluate models. For example, we specify the `extra_metrics` argument in the `leaderboard` method. You will find a new column whose name is the one we specified in `make_scorer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "675facf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T21:50:38.708003Z",
     "start_time": "2022-07-12T21:50:36.847428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>0.964965</td>\n",
       "      <td>0.298955</td>\n",
       "      <td>0.050804</td>\n",
       "      <td>15.585609</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.575590</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.9456</td>\n",
       "      <td>0.9456</td>\n",
       "      <td>0.955956</td>\n",
       "      <td>0.072992</td>\n",
       "      <td>0.025439</td>\n",
       "      <td>4.202675</td>\n",
       "      <td>0.072992</td>\n",
       "      <td>0.025439</td>\n",
       "      <td>4.202675</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.956957</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.025232</td>\n",
       "      <td>6.077209</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.025232</td>\n",
       "      <td>6.077209</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>0.949950</td>\n",
       "      <td>0.145704</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>9.428970</td>\n",
       "      <td>0.145704</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>9.428970</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>0.955956</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.009241</td>\n",
       "      <td>18.190912</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>0.009241</td>\n",
       "      <td>18.190912</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  accuracy  score_val  pred_time_test  \\\n",
       "0  WeightedEnsemble_L2      0.9500    0.9500   0.964965        0.298955   \n",
       "1             LightGBM      0.9456    0.9456   0.955956        0.072992   \n",
       "2              XGBoost      0.9448    0.9448   0.956957        0.064141   \n",
       "3        LightGBMLarge      0.9444    0.9444   0.949950        0.145704   \n",
       "4             CatBoost      0.9432    0.9432   0.955956        0.022018   \n",
       "\n",
       "   pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       0.050804  15.585609                 0.005762                0.000497   \n",
       "1       0.025439   4.202675                 0.072992                0.025439   \n",
       "2       0.025232   6.077209                 0.064141                0.025232   \n",
       "3       0.027910   9.428970                 0.145704                0.027910   \n",
       "4       0.009241  18.190912                 0.022018                0.009241   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0           0.575590            2       True         14  \n",
       "1           4.202675            1       True          5  \n",
       "2           6.077209            1       True         11  \n",
       "3           9.428970            1       True         13  \n",
       "4          18.190912            1       True          8  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(test_data, extra_metrics=[my_accuracy_ag], \n",
    "                      silent=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206fe69",
   "metadata": {},
   "source": [
    "Beyond implementing metrics from scratch, we can wrap metrics from other libraries. Here are examples to wrap scikit-learn metrics. The first is the MSE loss for regression, whose optimal value is 0 and a smaller value is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18b1a101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T23:05:14.244917Z",
     "start_time": "2022-07-12T23:05:14.241602Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "mse_ag = make_scorer(\n",
    "    name='mean_squared_error', score_func=sklearn.metrics.mean_squared_error,\n",
    "    optimum=0, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a9fc9",
   "metadata": {},
   "source": [
    "Then we wrap the area under the ROC curve for binary classification. Since we need multiple classification thresholds to compute the curve, we set `needs_threshold=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eba852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T23:02:46.177474Z",
     "start_time": "2022-07-12T23:02:46.174125Z"
    }
   },
   "outputs": [],
   "source": [
    "roc_auc_ag = make_scorer(\n",
    "    name='roc_auc', score_func=sklearn.metrics.roc_auc_score,\n",
    "    optimum=1, greater_is_better=True, needs_threshold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e076029",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
