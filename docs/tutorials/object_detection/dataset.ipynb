{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f47288a",
   "metadata": {},
   "source": [
    "# Object Detection - Prepare Dataset for Object Detector\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/new/docs/tutorials/object_detection/dataset.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/new/docs/tutorials/object_detection/dataset.ipynb)\n",
    "\n",
    "\n",
    ":label:`sec_object_detection_dataset`\n",
    "\n",
    "\n",
    "Preparing dataset for object detection is slightly difference and more difficult than image prediction.\n",
    "\n",
    "Our goal in this tutorial is to introduce the simplest methods to initiate or load a object detection dataset for `autogluon.vision.ObjectDetector`.\n",
    "\n",
    "There are generally two ways to load a dataset for ObjectDetector:\n",
    "\n",
    "- Load an existing object detection dataset, in VOC or COCO formats, downloaded or exported by other labeling tools.\n",
    "\n",
    "- Manually convert raw annotations in any format, knowing this you will be able to deal with arbitrary dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72eaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import autogluon.core as ag\n",
    "from autogluon.vision import ObjectDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e46e11",
   "metadata": {},
   "source": [
    "## Load an existing object detection dataset\n",
    "\n",
    "Pascal VOC and MS COCO are two most popular data format for object detection. Most public available object detection datasets follow either one of these two formats. In this tutorial we will not touch the details. You may view the original introduction for [VOC](http://host.robots.ox.ac.uk/pascal/VOC/) and [COCO](https://cocodataset.org/#home).\n",
    "\n",
    "To distinguish these two formats, you can either refer to the labeling tool or check the folder structure. Usually annotations in VOC format are individual `xml` files, while COCO format use a single `json` file to store all annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba676f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://autogluon.s3.amazonaws.com/datasets/tiny_motorbike.zip'\n",
    "dataset_train = ObjectDetector.Dataset.from_voc(url, splits='trainval')\n",
    "# or load from coco format, skip as it's too big to download\n",
    "# dataset_train = ObjectDetector.Dataset.from_coco(annotation_json_file, root='/path/to/root')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a505a8c",
   "metadata": {},
   "source": [
    "## Manually convert any format to autogluon object detector dataset\n",
    "\n",
    "We will walk you through by creating a dataset manually to help you understand the meaning of underlying data, this does not mean you have to do so. We highly recommend you to use a handy labeling tool for object detection if you want to create one by your own. Labeling bounding boxes are time consuming so a nice UI/UX design will significantly reduce the trouble.\n",
    "\n",
    "In the following section, we will use a single image and add annotations manually for all three major objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag.utils.download('https://raw.githubusercontent.com/zhreshold/mxnet-ssd/master/data/demo/dog.jpg', path='dog.jpg')\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "img = mpimg.imread('dog.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67410b19",
   "metadata": {},
   "source": [
    "With the grid on, we can roughly annotate this image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class NaiveDetectionGT:\n",
    "    def __init__(self, image):\n",
    "        self._objects = []\n",
    "        self.image = image\n",
    "        img = mpimg.imread('dog.jpg')\n",
    "        self.w = img.shape[1]\n",
    "        self.h = img.shape[0]\n",
    "\n",
    "    def add_object(self, name, xmin, ymin, xmax, ymax, difficult=0):\n",
    "        self._objects.append({'image': self.image, 'class': name,\n",
    "                              'xmin': xmin / self.w, 'ymin': ymin / self.h,\n",
    "                              'xmax': xmax / self.w, 'ymax': ymax / self.h, 'difficult': difficult})\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return pd.DataFrame(self._objects)\n",
    "\n",
    "gt = NaiveDetectionGT('dog.jpg')\n",
    "gt.add_object('dog', 140, 220, 300, 540)\n",
    "gt.add_object('bicycle', 120, 140, 580, 420)\n",
    "gt.add_object('car', 460, 70, 680, 170)\n",
    "df = gt.df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10701817",
   "metadata": {},
   "source": [
    "The `df` is a valid dataset and can be used by `ObjectDetector.fit` function. Internally it will be converted to object detection dataset, or you can manually convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ObjectDetector.Dataset(df, classes=df['class'].unique().tolist())\n",
    "dataset.show_images(nsample=1, ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520505bc",
   "metadata": {},
   "source": [
    "Congratulations, you can now proceed to :ref:`sec_object_detection_quick` to start training the `ObjectDetector`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}