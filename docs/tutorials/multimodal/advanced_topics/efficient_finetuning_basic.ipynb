{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a089589",
   "metadata": {},
   "source": [
    "# Single GPU Billion-scale Model Training via Parameter-Efficient Finetuning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/stable/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/stable/docs/tutorials/multimodal/advanced_topics/efficient_finetuning_basic.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "As pointed out by [a recent paper from Stanford Institute for Human-Centered Artificial Intelligence](https://arxiv.org/pdf/2108.07258.pdf), \n",
    "AI is undergoing a paradigm shift with the rise of \"foundation models\", i.e., giant models that are trained on a diverse collection of datasets generally in a self-supervised way. \n",
    "These foundation models, which are the key of AutoMM, can be easily adapted to down-stream applications. However, as the size of these foundation models grows, finetuning these models becomes increasingly difficult. \n",
    "Following is a figure from the [Microsoft research blog](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) that demonstrates the trend:\n",
    "\n",
    "![Scaling of foundation models](https://www.microsoft.com/en-us/research/uploads/prod/2021/10/model-size-graph.jpg)\n",
    "\n",
    "\n",
    "The goal of AutoMM is to help anyone solve machine learning problems via open source foundation models, including these giant models. \n",
    "To finetune these large-scale models, we adopt the recently popularized **parameter-efficient finetuning** technique. \n",
    "The idea is to either finetune a small subset of the weights in the foundation model (e.g., [BitFit](https://aclanthology.org/2022.acl-short.1.pdf)), \n",
    "or adding a tiny tunable structure on top of the fixed backbone (e.g., [Prompt Tuning](https://aclanthology.org/2021.emnlp-main.243.pdf),\n",
    "[LoRA](https://arxiv.org/pdf/2106.09685.pdf), [Adapter](https://arxiv.org/abs/1902.00751), [MAM Adapter](https://arxiv.org/pdf/2110.04366.pdf), [IA^3](https://arxiv.org/abs/2205.05638)). \n",
    "These techniques can effectively reduce the peak memory usage and model training time, while maintaining the performance.\n",
    "\n",
    "In this tutorial, we introduce how to apply parameter-efficient finetuning in MultiModalPredictor. \n",
    "We first introduce how to adopt the `\"ia3_bias\"` algorithm for parameter-efficient finetuning. Afterwards, we show how you can simply combine `\"ia3_bias\"` \n",
    "and gradient checkpointing to finetune the XL-variant of Google's [FLAN-T5](https://arxiv.org/abs/2210.11416) via a single NVIDIA T4 GPU. \n",
    "\n",
    "\n",
    "## Prepare Dataset\n",
    "\n",
    "The [Cross-Lingual Amazon Product Review Sentiment](https://webis.de/data/webis-cls-10.html) dataset contains Amazon product reviews in four languages. \n",
    "Here, we load the English and German fold of the dataset. In the label column, `0` means negative sentiment and `1` means positive sentiment. \n",
    "For the purpose of demonstration, we downsampled the training data to 1000 samples. We will train the model on the English dataset and \n",
    "directly evaluate its performance on the German and Japanese test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00faab-252f-44c9-b8f7-57131aa8251c",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install autogluon.multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457b7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet https://automl-mm-bench.s3.amazonaws.com/multilingual-datasets/amazon_review_sentiment_cross_lingual.zip -O amazon_review_sentiment_cross_lingual.zip\n",
    "!unzip -q -o amazon_review_sentiment_cross_lingual.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"cache\"\n",
    "\n",
    "def clear_cache():\n",
    "    if os.path.exists(\"cache\"):\n",
    "        shutil.rmtree(\"cache\")\n",
    "\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee45f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_en_df = pd.read_csv(\"amazon_review_sentiment_cross_lingual/en_train.tsv\",\n",
    "                          sep=\"\\t\",\n",
    "                          header=None,\n",
    "                          names=[\"label\", \"text\"]) \\\n",
    "                .sample(1000, random_state=123).reset_index(drop=True)\n",
    "\n",
    "test_en_df = pd.read_csv(\"amazon_review_sentiment_cross_lingual/en_test.tsv\",\n",
    "                          sep=\"\\t\",\n",
    "                          header=None,\n",
    "                          names=[\"label\", \"text\"]) \\\n",
    "               .sample(200, random_state=123).reset_index(drop=True)\n",
    "test_de_df = pd.read_csv(\"amazon_review_sentiment_cross_lingual/de_test.tsv\",\n",
    "                          sep=\"\\t\", header=None, names=[\"label\", \"text\"]) \\\n",
    "               .sample(200, random_state=123).reset_index(drop=True)\n",
    "\n",
    "test_jp_df = pd.read_csv('amazon_review_sentiment_cross_lingual/jp_test.tsv',\n",
    "                          sep='\\t', header=None, names=['label', 'text']) \\\n",
    "               .sample(200, random_state=123).reset_index(drop=True)\n",
    "train_en_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jp_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a23100",
   "metadata": {},
   "source": [
    "## Finetuning Multilingual Model with IA3 + BitFit\n",
    "\n",
    "In AutoMM, to enable efficient finetuning, just specify the `optim.peft` to be `\"ia3_bias\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a9ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import uuid\n",
    "\n",
    "model_path = f\"./tmp/{uuid.uuid4().hex}-multilingual_ia3\"\n",
    "predictor = MultiModalPredictor(label=\"label\",\n",
    "                                path=model_path)\n",
    "predictor.fit(train_en_df,\n",
    "              presets=\"multilingual\",\n",
    "              hyperparameters={\n",
    "                  \"optim.peft\": \"ia3_bias\",\n",
    "                  \"optim.lr_decay\": 0.9,\n",
    "                  \"optim.lr\": 3e-03,\n",
    "                  \"optim.end_lr\": 3e-03,\n",
    "                  \"optim.max_epochs\": 2,\n",
    "                  \"optim.warmup_steps\": 0,\n",
    "                  \"env.batch_size\": 32,\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365a4b0",
   "metadata": {},
   "source": [
    "The fraction of the tunable parameters is around **0.5%** of all parameters. Actually, the model trained purely on English data can achieve good performance \n",
    "on the test sets, even on the German / Japanese test set. It obtained **comparable results** as full-finetuning as in [AutoMM for Text - Multilingual Problems](../text_prediction/multilingual_text.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c010adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_in_en = predictor.evaluate(test_en_df)\n",
    "score_in_de = predictor.evaluate(test_de_df)\n",
    "score_in_jp = predictor.evaluate(test_jp_df)\n",
    "print('Score in the English Testset:', score_in_en)\n",
    "print('Score in the German Testset:', score_in_de)\n",
    "print('Score in the Japanese Testset:', score_in_jp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f816fd",
   "metadata": {},
   "source": [
    "## Training FLAN-T5-XL on Single GPU\n",
    "\n",
    "By combining [gradient checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and parameter-efficient finetuning, it is feasible to finetune \n",
    "[google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl) that has close to two billion parameterswith a single T4 GPU available in\n",
    "[AWS G4 instances](https://aws.amazon.com/ec2/instance-types/g4/). \n",
    "To turn on gradient checkpointing, you just need to set `\"model.hf_text.gradient_checkpointing\"` to `True`. \n",
    "To accelerate the training, we downsample the number of training samples to be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for clean the space\n",
    "clear_cache()\n",
    "shutil.rmtree(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aae890",
   "metadata": {},
   "source": [
    "```python\n",
    "from autogluon.multimodal import MultiModalPredictor\n",
    "\n",
    "train_en_df_downsample = train_en_df.sample(200, random_state=123)\n",
    "\n",
    "new_model_path = f\"./tmp/{uuid.uuid4().hex}-multilingual_ia3_gradient_checkpoint\"\n",
    "predictor = MultiModalPredictor(label=\"label\",\n",
    "                                path=new_model_path)\n",
    "predictor.fit(train_en_df_downsample,\n",
    "              presets=\"multilingual\",\n",
    "              hyperparameters={\n",
    "                  \"model.hf_text.checkpoint_name\": \"google/flan-t5-xl\",\n",
    "                  \"model.hf_text.gradient_checkpointing\": True,\n",
    "                  \"model.hf_text.low_cpu_mem_usage\": True,\n",
    "                  \"optim.peft\": \"ia3_bias\",\n",
    "                  \"optim.lr_decay\": 0.9,\n",
    "                  \"optim.lr\": 3e-03,\n",
    "                  \"optim.end_lr\": 3e-03,\n",
    "                  \"optim.max_epochs\": 1,\n",
    "                  \"optim.warmup_steps\": 0,\n",
    "                  \"env.batch_size\": 1,\n",
    "                  \"env.inference_batch_size_ratio\": 1\n",
    "              })\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd541505",
   "metadata": {},
   "source": [
    "```\n",
    "Global seed set to 123\n",
    "Auto select gpus: [0]\n",
    "GPU available: True (cuda), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
    "\n",
    "  | Name              | Type                         | Params\n",
    "-------------------------------------------------------------------\n",
    "0 | model             | HFAutoModelForTextPrediction | 1.2 B \n",
    "1 | validation_metric | AUROC                        | 0     \n",
    "2 | loss_func         | CrossEntropyLoss             | 0     \n",
    "-------------------------------------------------------------------\n",
    "203 K     Trainable params\n",
    "1.2 B     Non-trainable params\n",
    "1.2 B     Total params\n",
    "4,894.913 Total estimated model params size (MB)\n",
    "Epoch 0, global step 20: 'val_roc_auc' reached 0.88802 (best 0.88802), saving model to '/home/ubuntu/autogluon/docs/tutorials/multimodal/advanced_topics/multilingual_ia3_gradient_checkpoint/epoch=0-step=20.ckpt' as top 1\n",
    "Epoch 0, global step 40: 'val_roc_auc' reached 0.94531 (best 0.94531), saving model to '/home/ubuntu/autogluon/docs/tutorials/multimodal/advanced_topics/multilingual_ia3_gradient_checkpoint/epoch=0-step=40.ckpt' as top 1\n",
    "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<autogluon.multimodal.predictor.MultiModalPredictor at 0x7fd58c4dbca0>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ece044",
   "metadata": {},
   "source": [
    "```python\n",
    "score_in_en = predictor.evaluate(test_en_df)\n",
    "print('Score in the English Testset:', score_in_en)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dabce1",
   "metadata": {},
   "source": [
    "```\n",
    "Score in the English Testset: {'roc_auc': 0.931263189629183}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106bdc64",
   "metadata": {},
   "source": [
    "```python\n",
    "# Just for clean the space\n",
    "clear_cache()\n",
    "shutil.rmtree(new_model_path)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b88c8",
   "metadata": {},
   "source": [
    "## Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/autogluon/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "## Customization\n",
    "To learn how to customize AutoMM, please refer to [Customize AutoMM](customization.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}