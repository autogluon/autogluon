{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ae309b-3d20-4cd2-b555-17a1e7276e65",
   "metadata": {},
   "source": [
    "# Faster Prediction with TensorRT\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/tensorrt.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69651c7-fd60-4856-ae78-556a77d7c2e6",
   "metadata": {},
   "source": [
    "AutoMM is a deep learning \"model zoo\" of model zoos. It can automatically build deep learning models that are suitable for multimodal datasets. You will only need to convert the data into the multimodal dataframe format\n",
    "and AutoMM can predict the values of one column conditioned on the features from the other columns including images, text, and tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9610bcb-a916-494d-9dea-f5f2dd4ec64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5757d-fda3-4c03-8d97-e71c828e34cd",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For demonstration, we use a simplified and subsampled version of [PetFinder dataset](https://www.kaggle.com/c/petfinder-adoption-prediction). The task is to predict the animals' adoption rates based on their adoption profile information. In this simplified version, the adoption speed is grouped into two categories: 0 (slow) and 1 (fast).\n",
    "\n",
    "To get started, let's download and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d6cbe-5d84-405c-b03c-367d6c019ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = './ag_automm_tutorial'\n",
    "zip_file = 'https://automl-mm-bench.s3.amazonaws.com/petfinder_for_tutorial.zip'\n",
    "from autogluon.core.utils.loaders import load_zip\n",
    "load_zip.unzip(zip_file, unzip_dir=download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fa71d-e7ea-4e17-8d99-c9eadfeb437a",
   "metadata": {},
   "source": [
    "Next, we will load the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d7557-bf66-4e37-959d-10c542b5070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_path = download_dir + '/petfinder_for_tutorial'\n",
    "train_data = pd.read_csv(f'{dataset_path}/train.csv', index_col=0)\n",
    "test_data = pd.read_csv(f'{dataset_path}/test.csv', index_col=0)\n",
    "label_col = 'AdoptionSpeed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449186e-fc1d-4d38-bce7-6352456dc44b",
   "metadata": {},
   "source": [
    "We need to expand the image paths to load them in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f74f73-118c-4521-bd6e-dd8f4c0afe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_col = 'Images'\n",
    "train_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0]) # Use the first image for a quick tutorial\n",
    "test_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0])\n",
    "\n",
    "def path_expander(path, base_folder):\n",
    "    path_l = path.split(';')\n",
    "    return ';'.join([os.path.abspath(os.path.join(base_folder, path)) for path in path_l])\n",
    "\n",
    "train_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n",
    "test_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7624cc-98fa-4cae-ad8f-d72a864378be",
   "metadata": {},
   "source": [
    "Each animal's adoption profile includes pictures, a text description, and various tabular features such as age, breed, name, color, and more. Refer to :ref:`sec_automm_multimodal_beginner` for visualization of an example row of the dataset.\n",
    "\n",
    "## Training\n",
    "Now let's fit the predictor with the training data. Here we set a tight time budget for a quick demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054357e-de46-4817-9632-f3ac6af2db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "predictor = MultiModalPredictor(label=label_col).fit(\n",
    "    train_data=train_data,\n",
    "    time_limit=120, # seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b458c00-a75f-4903-b8a6-020b32d29ad7",
   "metadata": {},
   "source": [
    "Under the hood, AutoMM automatically infers the problem type (classification or regression), detects the data modalities, selects the related models from the multimodal model pools, and trains the selected models. If multiple backbones are available, AutoMM appends a late-fusion model (MLP or transformer) on top of them.\n",
    "\n",
    "## Prediction with vanilla module\n",
    "Given a multimodal dataframe without the label column, we can predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63689e08-28f2-43f7-9f81-67cb3b5e0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [2, 4, 8]:\n",
    "    sample = test_data.head(batch_size)\n",
    "    for _ in range(3):\n",
    "        tic = time.time()\n",
    "        y_pred = predictor.predict(sample)\n",
    "        print(f\"elapsed (vanilla): {(time.time()-tic)*1000:.1f} ms (batch_size={batch_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37872072-0729-44d5-8273-00d1c06edb7e",
   "metadata": {},
   "source": [
    "## Prediction with TensorRT module\n",
    "First, we need to export the module to ONNX, in order to use TensorrtExecutionProvider in onnxruntime for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786d056-18a8-43aa-a2cd-9fb239db4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_data.head(2)\n",
    "trt_module = predictor.export_tensorrt(data=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb3fe6-c78c-49d3-9339-d15d9369e88d",
   "metadata": {},
   "source": [
    "The exported OnnxModule can be a drop-in replacement of torch.nn.Module. Therefore, we can replace the internal neural network module directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98bb16-038c-4878-ac17-886762c8ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor._model = trt_module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7cde9-044a-44b8-9269-c279e14f1cc8",
   "metadata": {},
   "source": [
    "Then, we can perform prediction or extract embeddings as usual. To verify dynamic shape support, we can predict with varying batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5697adf-7dea-4006-afe3-56b0981f6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [2, 4, 8]:\n",
    "    sample = test_data.head(batch_size)\n",
    "    for _ in range(3):\n",
    "        tic = time.time()\n",
    "        y_pred_trt = predictor.predict(sample)\n",
    "        print(f\"elapsed (tensorrt): {(time.time()-tic)*1000:.1f} ms (batch_size={batch_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce7ab8-bafb-462d-8207-d08aa0ffcfb3",
   "metadata": {},
   "source": [
    "To verify the correctness of the prediction results, we can compare the results side-by-side.\n",
    "\n",
    "Let's print the expected results first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1c445-fc86-46b0-b0b7-31d6093e9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f629185-148e-477a-9360-f8a1dfd50659",
   "metadata": {},
   "source": [
    "Then the results from TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474398ed-3e3d-4c94-b12e-d98bd9b98eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_trt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff9598-a076-4aaf-a375-8a4768dd328a",
   "metadata": {},
   "source": [
    "We can safely assume these results are relatively close for most of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6483a6-2e4d-4b72-aaa5-a497b966a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(y_pred, y_pred_trt, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9cc93-5803-4a29-8e94-e90fe7ef45bb",
   "metadata": {},
   "source": [
    "## Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/autogluon/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "## Customization\n",
    "To learn how to customize AutoMM, please refer to :ref:`sec_automm_customization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea505299-67a1-4a09-a49c-ac4cef40d5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
