{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22771bcc-be48-4bc6-906e-e450568a8734",
   "metadata": {},
   "source": [
    "# Few Shot Learning with `FewShotSVMPredictor`\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/advanced_topics/few_shot_learning.ipynb)\n",
    "\n",
    "\n",
    "In this tutorial we introduce a simple but effective way for few shot classification problems. \n",
    "We present the FusionSVM model which leverages the high-quality features from foundational models and use a simple SVM for few shot classification task.\n",
    "Specifically, we extract sample features with pretrained models, and use the features for SVM learning.\n",
    "We show the effectiveness of this FusionSVMModel on a text classification dataset and a vision classification dataset. \n",
    "\n",
    "## Text Classification on MLDoc dataset\n",
    "### Load Dataset\n",
    "We prepare all datasets in the format of `pd.DataFrame` as in many of our tutorials have done. \n",
    "For this tutorial, we'll use a small `MLDoc` dataset for demonstration. \n",
    "The dataset is a text classification dataset, which contains 4 classes and we downsampled the training data to 10 samples per class, a.k.a 10 shots.\n",
    "For more details regarding `MLDoc` please see this [link](https://github.com/facebookresearch/MLDoc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00faab-252f-44c9-b8f7-57131aa8251c",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install autogluon.multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995899a-a489-4861-95f1-8312ed83f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from autogluon.core.utils.loaders import load_zip\n",
    "\n",
    "download_dir = \"./ag_automm_tutorial_fs_cls\"\n",
    "zip_file = \"https://automl-mm-bench.s3.amazonaws.com/nlp_datasets/MLDoc-10shot-en.zip\"\n",
    "load_zip.unzip(zip_file, unzip_dir=download_dir)\n",
    "dataset_path = os.path.join(download_dir)\n",
    "train_df = pd.read_csv(f\"{dataset_path}/train.csv\", names=[\"label\", \"text\"])\n",
    "test_df = pd.read_csv(f\"{dataset_path}/test.csv\", names=[\"label\", \"text\"])\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4fe3a-3acd-4db1-af27-e8aca839bb66",
   "metadata": {},
   "source": [
    "### Create the `FewShotSVMPredictor`\n",
    "In order to run FusionSVM model, we first initialize a `FewShotSVMPredictor` with the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393bd8f4-76bc-4889-ade2-e94c9d7374bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal.utils.few_shot_learning import FewShotSVMPredictor\n",
    "hyperparameters = {\n",
    "    \"model.hf_text.checkpoint_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"model.hf_text.pooling_mode\": \"mean\",\n",
    "    \"env.per_gpu_batch_size\": 32,\n",
    "    \"env.eval_batch_size_ratio\": 4,\n",
    "}\n",
    "\n",
    "import uuid\n",
    "model_path = f\"./tmp/{uuid.uuid4().hex}-automm_mldoc-10shot-en\"\n",
    "predictor = FewShotSVMPredictor(\n",
    "    label=\"label\",  # column name of the label\n",
    "    hyperparameters=hyperparameters,\n",
    "    eval_metric=\"acc\",\n",
    "    path=model_path  # path to save model and artifacts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d459d1-bdf6-4533-83c2-41a07765bc46",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Now we train the model with the `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc6144-ec6d-4541-921a-a5b2c052bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c7459-b4c9-4dc5-b01f-ac32623d3e1a",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb721f1-9179-498e-babb-b27a2cc9e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.evaluate(test_df, metrics=[\"acc\", \"macro_f1\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e47a0-21a9-4261-9bd2-5c172ba8fc5d",
   "metadata": {},
   "source": [
    "### Comparing to the normal `MultiModalPredictor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2aab3c-fcd7-4220-b1e8-d2799ecb9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model.hf_text.checkpoint_name\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"model.hf_text.pooling_mode\": \"mean\",\n",
    "}\n",
    "\n",
    "automm_predictor = MultiModalPredictor(\n",
    "    label=\"label\",\n",
    "    problem_type=\"classification\",\n",
    "    eval_metric=\"acc\"\n",
    ")\n",
    "\n",
    "automm_predictor.fit(\n",
    "    train_data=train_df,\n",
    "    presets=\"multilingual\",\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "results, preds = automm_predictor.evaluate(test_df, return_pred=True)\n",
    "test_labels = np.array(test_df[\"label\"])\n",
    "macro_f1 = f1_score(test_labels, preds, average=\"macro\")\n",
    "results[\"macro_f1\"] = macro_f1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385eec2-def2-4131-afe3-bec456cba679",
   "metadata": {},
   "source": [
    "As you can see that the `FewShotSVMPredictor` performs much better than the normal `MultiModalPredictor`. \n",
    "\n",
    "### Load a pretrained model\n",
    "The `FewShotSVMPredictor` automatically saves the model and artifacts to disk during training. \n",
    "You can specify the path to save by setting the `path=<your_desired_save_path>` when initializing the predictor.\n",
    "You can also load a pretrained `FewShotSVMPredictor` and perform downstream tasks by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda2278-9463-49c6-aea4-ae9827e4498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor2 = FewShotSVMPredictor.load(model_path)\n",
    "result2 = predictor2.evaluate(test_df, metrics=[\"acc\", \"macro_f1\"])\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dd113-01c9-48ee-8fd9-7475f6b5020e",
   "metadata": {},
   "source": [
    "## Image Classification on Stanford Cars\n",
    "\n",
    "### Load Dataset\n",
    "We also provide an example of using `FewShotSVMPredictor` on a few-shot image classification task. \n",
    "We use the Stanford Cars dataset for demonstration and downsampled the training set to have 8 samples per class.\n",
    "The Stanford Cars is an image classification dataset and contains 196 classes.\n",
    "For more information regarding the dataset, please see [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfb087-ac7a-4c61-83cc-45e0e030bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from autogluon.core.utils.loaders import load_zip, load_s3\n",
    "\n",
    "download_dir = \"./ag_automm_tutorial_fs_cls/stanfordcars/\"\n",
    "zip_file = \"https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/stanfordcars.zip\"\n",
    "train_csv = \"https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/train_8shot.csv\"\n",
    "test_csv = \"https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/test.csv\"\n",
    "\n",
    "load_zip.unzip(zip_file, unzip_dir=download_dir)\n",
    "dataset_path = os.path.join(download_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfb087-ac7a-4c61-83cc-45e0e030bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/train_8shot.csv -O ./ag_automm_tutorial_fs_cls/stanfordcars/train.csv\n",
    "!wget https://automl-mm-bench.s3.amazonaws.com/vision_datasets/stanfordcars/test.csv -O ./ag_automm_tutorial_fs_cls/stanfordcars/test.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfb087-ac7a-4c61-83cc-45e0e030bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df_raw = pd.read_csv(os.path.join(download_dir, \"train.csv\"))\n",
    "train_df = train_df_raw.drop(\n",
    "        columns=[\n",
    "            \"Source\",\n",
    "            \"Confidence\",\n",
    "            \"XMin\",\n",
    "            \"XMax\",\n",
    "            \"YMin\",\n",
    "            \"YMax\",\n",
    "            \"IsOccluded\",\n",
    "            \"IsTruncated\",\n",
    "            \"IsGroupOf\",\n",
    "            \"IsDepiction\",\n",
    "            \"IsInside\",\n",
    "        ]\n",
    "    )\n",
    "train_df[\"ImageID\"] = download_dir + train_df[\"ImageID\"].astype(str)\n",
    "\n",
    "\n",
    "test_df_raw = pd.read_csv(os.path.join(download_dir, \"test.csv\"))\n",
    "test_df = test_df_raw.drop(\n",
    "        columns=[\n",
    "            \"Source\",\n",
    "            \"Confidence\",\n",
    "            \"XMin\",\n",
    "            \"XMax\",\n",
    "            \"YMin\",\n",
    "            \"YMax\",\n",
    "            \"IsOccluded\",\n",
    "            \"IsTruncated\",\n",
    "            \"IsGroupOf\",\n",
    "            \"IsDepiction\",\n",
    "            \"IsInside\",\n",
    "        ]\n",
    "    )\n",
    "test_df[\"ImageID\"] = download_dir + test_df[\"ImageID\"].astype(str)\n",
    "\n",
    "print(os.path.exists(train_df.iloc[0][\"ImageID\"]))\n",
    "print(train_df)\n",
    "print(os.path.exists(test_df.iloc[0][\"ImageID\"]))\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476756e-38eb-49f9-941b-11a4027fb840",
   "metadata": {},
   "source": [
    "### Create the `FewShotSVMPredictor`\n",
    "In order to run FusionSVM model, we first initialize a `FewShotSVMPredictor` with the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f92fd-eb23-417e-9968-21db3af9a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal.utils.few_shot_learning import FewShotSVMPredictor\n",
    "hyperparameters = {\n",
    "    \"model.names\": [\"clip\"],\n",
    "    \"model.clip.max_text_len\": 0,\n",
    "    \"env.num_workers\": 2,\n",
    "    \"model.clip.checkpoint_name\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"env.eval_batch_size_ratio\": 1,\n",
    "}\n",
    "\n",
    "import uuid\n",
    "model_path = f\"./tmp/{uuid.uuid4().hex}-automm_stanfordcars-8shot-en\"\n",
    "predictor = FewShotSVMPredictor(\n",
    "    label=\"LabelName\",  # column name of the label\n",
    "    hyperparameters=hyperparameters,\n",
    "    eval_metric=\"acc\",\n",
    "    path=model_path  # path to save model and artifacts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bb49b-007b-4f5c-a551-7e8f9208234e",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Now we train the model with the `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90305f54-b468-4ad7-a08a-2547481a735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b33c5-aef7-470e-bc0b-93d316338a84",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0112f66-6290-43de-9124-116b762e4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.evaluate(test_df, metrics=[\"acc\", \"macro_f1\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953ec4e-7371-42af-ba29-6102d0d212c1",
   "metadata": {},
   "source": [
    "### Comparing to the normal `MultiModalPredictor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b390066-99fc-4cd8-b542-5e8f01ded73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model.names\": [\"timm_image\"],\n",
    "    \"model.timm_image.checkpoint_name\": \"swin_base_patch4_window7_224\",\n",
    "    \"env.per_gpu_batch_size\": 8,\n",
    "    \"optimization.max_epochs\": 10,\n",
    "    \"optimization.learning_rate\": 1.0e-3,\n",
    "    \"optimization.optim_type\": \"adamw\",\n",
    "    \"optimization.weight_decay\": 1.0e-3,\n",
    "}\n",
    "\n",
    "automm_predictor = MultiModalPredictor(\n",
    "    label=\"LabelName\",  # column name of the label\n",
    "    hyperparameters=hyperparameters,\n",
    "    problem_type=\"classification\",\n",
    "    eval_metric=\"acc\",\n",
    ")\n",
    "automm_predictor.fit(\n",
    "    train_data=train_df,\n",
    ")\n",
    "\n",
    "results, preds = automm_predictor.evaluate(test_df, return_pred=True)\n",
    "test_labels = np.array(test_df[\"LabelName\"])\n",
    "macro_f1 = f1_score(test_labels, preds, average=\"macro\")\n",
    "results[\"macro_f1\"] = macro_f1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf11ec9-2e9a-42a1-9dc0-69137dad2d5a",
   "metadata": {},
   "source": [
    "As you can see that the `FewShotSVMPredictor` performs much better than the normal `MultiModalPredictor` in image classification as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9ed50-7ab1-4f26-82fd-99670c7c4a4f",
   "metadata": {},
   "source": [
    "### Citation\n",
    "```\n",
    "@InProceedings{SCHWENK18.658,\n",
    "  author = {Holger Schwenk and Xian Li},\n",
    "  title = {A Corpus for Multilingual Document Classification in Eight Languages},\n",
    "  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},\n",
    "  year = {2018},\n",
    "  month = {may},\n",
    "  date = {7-12},\n",
    "  location = {Miyazaki, Japan},\n",
    "  editor = {Nicoletta Calzolari (Conference chair) and Khalid Choukri and Christopher Cieri and Thierry Declerck and Sara Goggi and Koiti Hasida and Hitoshi Isahara and Bente Maegaard and Joseph Mariani and Hélène Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis and Takenobu Tokunaga},\n",
    "  publisher = {European Language Resources Association (ELRA)},\n",
    "  address = {Paris, France},\n",
    "  isbn = {979-10-95546-00-9},\n",
    "  language = {english}\n",
    "  }\n",
    "  \n",
    "@inproceedings{KrauseStarkDengFei-Fei_3DRR2013,\n",
    "  title = {3D Object Representations for Fine-Grained Categorization},\n",
    "  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},\n",
    "  year = {2013},\n",
    "  address = {Sydney, Australia},\n",
    "  author = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}