{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a8be40",
   "metadata": {},
   "source": [
    "# AutoMM Detection - High Performance Finetune on COCO Format Dataset\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/finetune/detection_high_performance_finetune_coco.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/finetune/detection_high_performance_finetune_coco.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "![Pothole Dataset](https://automl-mm-bench.s3.amazonaws.com/object_detection/example_image/pothole144_gt.jpg)\n",
    "\n",
    "\n",
    "In this section, our goal is to fast finetune and evaluate a pretrained model \n",
    "on [Pothole dataset](https://www.kaggle.com/datasets/andrewmvd/pothole-detection) in COCO format.\n",
    "Pothole is a single object, i.e. `pothole`, detection dataset, containing 665 images with bounding box annotations\n",
    "for the creation of detection models and can work as POC/POV for the maintenance of roads.\n",
    "See [AutoMM Detection - Prepare Pothole Dataset](../data_preparation/prepare_pothole.ipynb) for how to prepare Pothole dataset.\n",
    "\n",
    "To start, let's import MultiModalPredictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00faab-252f-44c9-b8f7-57131aa8251c",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install autogluon.multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9870995",
   "metadata": {},
   "source": [
    "Make sure `mmcv-full` and `mmdet` are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e102d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mim install mmcv-full\n",
    "!pip install mmdet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccf074",
   "metadata": {},
   "source": [
    "And also import some other packages that will be used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae142b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from autogluon.core.utils.loaders import load_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a56955",
   "metadata": {},
   "source": [
    "We have the sample dataset ready in the cloud. Let's download it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = \"https://automl-mm-bench.s3.amazonaws.com/object_detection/dataset/pothole.zip\"\n",
    "download_dir = \"./pothole\"\n",
    "\n",
    "load_zip.unzip(zip_file, unzip_dir=download_dir)\n",
    "data_dir = os.path.join(download_dir, \"pothole\")\n",
    "train_path = os.path.join(data_dir, \"Annotations\", \"usersplit_train_cocoformat.json\")\n",
    "val_path = os.path.join(data_dir, \"Annotations\", \"usersplit_val_cocoformat.json\")\n",
    "test_path = os.path.join(data_dir, \"Annotations\", \"usersplit_test_cocoformat.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2155682",
   "metadata": {},
   "source": [
    "While using COCO format dataset, the input is the json annotation file of the dataset split.\n",
    "In this example, `usersplit_train_cocoformat.json` is the annotation file of the train split.\n",
    "`usersplit_val_cocoformat.json` is the annotation file of the validation split.\n",
    "And `usersplit_test_cocoformat.json` is the annotation file of the test split.\n",
    "\n",
    "We select the VFNet with ResNet-50 as backbone, Feature Pyramid Network (FPN) as neck,\n",
    "and input resolution is 640x640, pretrained on COCO dataset.\n",
    "*(The neck of the object detector refers to the additional layers existing between the backbone and the head. \n",
    "Their role is to collect feature maps from different stages.)*\n",
    "With this setting, it sacrifices training and inference time,\n",
    "and also requires much more GPU memory,\n",
    "but the performance is high. \n",
    "\n",
    "We use `val_metric = map`, i.e., mean average precision in COCO standard as our validation metric.\n",
    "In previous section [AutoMM Detection - Fast Finetune on COCO Format Dataset](detection_fast_finetune_coco.ipynb),\n",
    "we did not specify the validation metric and by default the validation loss is used as validation metric.\n",
    "Using validation loss is much faster but using mean average precision gives the best performance.\n",
    "\n",
    "And we use all the GPUs (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco\"\n",
    "num_gpus = -1  # use all GPUs\n",
    "val_metric = \"map\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ed93b",
   "metadata": {},
   "source": [
    "We create the MultiModalPredictor with selected checkpoint name, val_metric, and number of GPUs.\n",
    "We need to specify the problem_type to `\"object_detection\"`,\n",
    "and also provide a `sample_data_path` for the predictor to infer the categories of the dataset.\n",
    "Here we provide the `train_path`, and it also works using any other split of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MultiModalPredictor(\n",
    "    hyperparameters={\n",
    "        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n",
    "        \"env.num_gpus\": num_gpus,\n",
    "        \"optimization.val_metric\": val_metric,\n",
    "    },\n",
    "    problem_type=\"object_detection\",\n",
    "    sample_data_path=train_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13325b2a",
   "metadata": {},
   "source": [
    "We used `1e-4` for Yolo V3 in previous tutorial, \n",
    "but set the learning rate to be `5e-6` here, \n",
    "because larger model always prefers smaller learning rate.\n",
    "Note that we use a two-stage learning rate option during finetuning by default,\n",
    "and the model head will have 100x learning rate.\n",
    "Using a two-stage learning rate with high learning rate only on head layers makes\n",
    "the model converge faster during finetuning. It usually gives better performance as well,\n",
    "especially on small datasets with hundreds or thousands of images.\n",
    "We also set the batch_size to be 2, because this model is too huge to run with larger batch size.\n",
    "We also compute the time of the fit process here for better understanding the speed.\n",
    "We only set the number of epochs to be 1 for a quick demonstration, \n",
    "and to seriously finetune the model on this dataset we will need to set this to 20 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "predictor.fit(\n",
    "    train_path,\n",
    "    hyperparameters={\n",
    "        \"optimization.learning_rate\": 5e-6, # we use two stage and detection head has 100x lr\n",
    "        \"optimization.max_epochs\": 1,\n",
    "        \"optimization.check_val_every_n_epoch\": 1, # make sure there is at least one validation\n",
    "        \"env.per_gpu_batch_size\": 2,  # decrease it when model is large\n",
    "    },\n",
    ")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb52bc4",
   "metadata": {},
   "source": [
    "Print out the time and we can see that it takes a long time even for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac127735",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This finetuning takes %.2f seconds.\" % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c04be",
   "metadata": {},
   "source": [
    "To get a model with reasonable performance, you will need to finetune the model with more epochs.\n",
    "We set `max_epochs` to 50 and trained a model offline. And we uploaded it to AWS S3. \n",
    "To load and check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Predictor from S3\n",
    "zip_file = \"https://automl-mm-bench.s3.amazonaws.com/object_detection/checkpoints/pothole_AP50_718.zip\"\n",
    "download_dir = \"./pothole_AP50_718\"\n",
    "load_zip.unzip(zip_file, unzip_dir=download_dir)\n",
    "better_predictor = MultiModalPredictor.load(\"./pothole_AP50_718/AutogluonModels/ag-20221123_021130\")\n",
    "better_predictor.set_num_gpus(1)\n",
    "\n",
    "# Evaluate new predictor\n",
    "better_predictor.evaluate(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523da2c",
   "metadata": {},
   "source": [
    "We can get the prediction on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea88eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = better_predictor.predict(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d6f8e",
   "metadata": {},
   "source": [
    "Let's also visualize the prediction result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6caffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal.utils import visualize_detection\n",
    "conf_threshold = 0.25  # Specify a confidence threshold to filter out unwanted boxes\n",
    "visualization_result_dir = \"./\"  # Use the pwd as result dir to save the visualized image\n",
    "visualized = visualize_detection(\n",
    "    pred=pred[12:13],\n",
    "    detection_classes=predictor.get_predictor_classes(),\n",
    "    conf_threshold=conf_threshold,\n",
    "    visualization_result_dir=visualization_result_dir,\n",
    ")\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "img = Image.fromarray(visualized[0][:, :, ::-1], 'RGB')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759a35d",
   "metadata": {},
   "source": [
    "Under this high performance finetune setting, it took a long time to train but reached `mAP = 0.450, mAP50 = 0.718`!\n",
    "For how to finetune faster,\n",
    "see [AutoMM Detection - Fast Finetune on COCO Format Dataset](detection_fast_finetune_coco.ipynb), where we finetuned a YOLOv3 model with lower\n",
    "performance but much faster.\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/autogluon/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "## Customization\n",
    "To learn how to customize AutoMM, please refer to [Customize AutoMM](../../advanced_topics/customization.ipynb).\n",
    "\n",
    "## Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a47424",
   "metadata": {},
   "source": [
    "```\n",
    "@article{DBLP:journals/corr/abs-2008-13367,\n",
    "  author    = {Haoyang Zhang and\n",
    "               Ying Wang and\n",
    "               Feras Dayoub and\n",
    "               Niko S{\\\"{u}}nderhauf},\n",
    "  title     = {VarifocalNet: An IoU-aware Dense Object Detector},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/2008.13367},\n",
    "  year      = {2020},\n",
    "  url       = {https://arxiv.org/abs/2008.13367},\n",
    "  eprinttype = {arXiv},\n",
    "  eprint    = {2008.13367},\n",
    "  timestamp = {Wed, 16 Sep 2020 11:20:03 +0200},\n",
    "  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-13367.bib},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}