{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f900e7",
   "metadata": {},
   "source": [
    "# AutoMM Detection - Evaluate Pretrained Faster R-CNN on VOC Format Dataset\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/evaluation/detection_eval_fasterrcnn_voc.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/object_detection/evaluation/detection_eval_fasterrcnn_voc.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "In this section, our goal is to evaluate Faster-RCNN model on VOC2007 dataset in VOC format.\n",
    "See [AutoMM Detection - Convert VOC Format Dataset to COCO Format](../data_preparation/voc_to_coco.ipynb) for how to quickly convert a VOC format dataset.\n",
    "In previous section [AutoMM Detection - Evaluate Pretrained Faster R-CNN on COCO Format Dataset](detection_eval_fasterrcnn_coco.ipynb), we evaluated Faster-RCNN on COCO dataset.\n",
    "We strongly recommend using COCO format, but AutoMM still have limited support for VOC format for quick proof testing.\n",
    "\n",
    "To start, let's import MultiModalPredictor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14dbee5",
   "metadata": {},
   "source": [
    "```python\n",
    "from autogluon.multimodal import MultiModalPredictor\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c621878",
   "metadata": {},
   "source": [
    "We use the Faster R-CNN with ResNet50 as backbone and Feature Pyramid Network (FPN) as neck.\n",
    "This is the only model we support that is pretrained on VOC.\n",
    "It's always recommended to finetune a model pretrained on COCO which is a larger dataset with more complicated task.\n",
    "To test other model structures on VOC, check [AutoMM Detection - Convert VOC Format Dataset to COCO Format](../data_preparation/voc_to_coco.ipynb) and [AutoMM Detection - Fast Finetune on COCO Format Dataset](../finetune/detection_fast_finetune_coco.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2196c",
   "metadata": {},
   "source": [
    "```python\n",
    "checkpoint_name = \"faster_rcnn_r50_fpn_1x_voc0712\"\n",
    "num_gpus = 1  # multi GPU inference is not supported in VOC format\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7737c",
   "metadata": {},
   "source": [
    "As before, we create the MultiModalPredictor with selected checkpoint name and number of GPUs.\n",
    "We also need to specify the problem_type to `\"object_detection\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a9f9f",
   "metadata": {},
   "source": [
    "```python\n",
    "predictor = MultiModalPredictor(\n",
    "    hyperparameters={\n",
    "        \"model.mmdet_image.checkpoint_name\": checkpoint_name,\n",
    "        \"env.num_gpus\": num_gpus,\n",
    "    },\n",
    "    problem_type=\"object_detection\",\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb878d48",
   "metadata": {},
   "source": [
    "Here we use VOC2007 for testing: [AutoMM Detection - Prepare Pascal VOC Dataset](../data_preparation/prepare_voc.ipynb).\n",
    "While using VOC format dataset, the input is the root path of the dataset, and contains at least:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc465853",
   "metadata": {},
   "source": [
    "```\n",
    "Annotations  ImageSets  JPEGImages labels.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb88c53",
   "metadata": {},
   "source": [
    "Here `labels.txt` shall be added manually to include all the labels in the dataset. \n",
    "In this example, the content of `labels.txt` is shown as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff9926",
   "metadata": {},
   "source": [
    "```\n",
    "aeroplane\n",
    "bicycle\n",
    "bird\n",
    "boat\n",
    "bottle\n",
    "bus\n",
    "car\n",
    "cat\n",
    "chair\n",
    "cow\n",
    "diningtable\n",
    "dog\n",
    "horse\n",
    "motorbike\n",
    "person\n",
    "pottedplant\n",
    "sheep\n",
    "sofa\n",
    "train\n",
    "tvmonitor\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e646f0",
   "metadata": {},
   "source": [
    "For VOC format data, we always use root_path. And the predictor will automatically select the split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcc789",
   "metadata": {},
   "source": [
    "```python\n",
    "test_path = \"VOCdevkit/VOC2007\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a2086",
   "metadata": {},
   "source": [
    "To evaluate the pretrained Faster R-CNN model we loaded, run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541a318",
   "metadata": {},
   "source": [
    "```python\n",
    "result = predictor.evaluate(test_path)\n",
    "print(result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6efc6",
   "metadata": {},
   "source": [
    "Here the test set is selected automatically in `predictor.evaluate`.\n",
    "And if we `print(result)`, the first value `0.4406` is mAP in COCO standard, and the second one `0.7328` is mAP in VOC standard (or mAP50). For more details about these metrics, see [COCO's evaluation guideline](https://cocodataset.org/#detection-eval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69cbe8",
   "metadata": {},
   "source": [
    "```\n",
    "{'map': tensor(0.4406), 'map_50': tensor(0.7328), 'map_75': tensor(0.4658), 'map_small': tensor(0.0959), 'map_medium': tensor(0.3085), 'map_large': tensor(0.5281), 'mar_1': tensor(0.3761), 'mar_10': tensor(0.5297), 'mar_100': tensor(0.5368), 'mar_small': tensor(0.1485), 'mar_medium': tensor(0.4192), 'mar_large': tensor(0.6328), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.)}\n",
    "time usage: 533.67\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644d82f",
   "metadata": {},
   "source": [
    "## Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/autogluon/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "## Customization\n",
    "To learn how to customize AutoMM, please refer to [Customize AutoMM](../../advanced_topics/customization.ipynb).\n",
    "\n",
    "## Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1a75e",
   "metadata": {},
   "source": [
    "```\n",
    "@article{Ren_2017,\n",
    "   title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},\n",
    "   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n",
    "   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n",
    "   author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},\n",
    "   year={2017},\n",
    "   month={Jun},\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}