{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5761b4",
   "metadata": {},
   "source": [
    "# Zero-Shot Image Classification with CLIP\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/master/docs/tutorials/multimodal/image_prediction/clip_zeroshot.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "When you want to classify an image to different classes, it is standard to train an image classifier based on the class names. However, it is tedious to collect training data. And if the collected data is too few or too imbalanced, you may not get a decent image classifier. So you wonder, is there a strong enough model that can handle this situaton without the training efforts?\n",
    "\n",
    "Actually there is! OpenAI has introduced a model named [CLIP](https://openai.com/blog/clip/), which can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized. And its accuracy is high, e.g., CLIP can achieve 76.2% top-1 accuracy on ImageNet without using any of the 1.28M training samples. This performance matches with original supervised ResNet50 on ImageNet, quite promising for a classification task with 1000 classes!\n",
    "\n",
    "So in this tutorial, let's dive deep into CLIP. We will show you how to use CLIP model to do zero-shot image classification in AutoGluon.\n",
    "\n",
    "## Simple Demo\n",
    "\n",
    "Here we provide a simple demo to classify what dog breed is in the picture below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00faab-252f-44c9-b8f7-57131aa8251c",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install autogluon.multimodal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from autogluon.multimodal import download\n",
    "\n",
    "url = \"https://farm4.staticflickr.com/3445/3262471985_ed886bf61a_z.jpg\"\n",
    "dog_image = download(url)\n",
    "\n",
    "pil_img = Image(filename=dog_image)\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa2d97",
   "metadata": {},
   "source": [
    "Normally to solve this task, you need to collect some training data (e.g., [the Stanford Dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/)) and train a dog breed classifier. But with CLIP, all you need to do is provide some potential visual categories. CLIP will handle the rest for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee0b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "\n",
    "predictor = MultiModalPredictor(problem_type=\"zero_shot_image_classification\")\n",
    "prob = predictor.predict_proba({\"image\": [dog_image]}, {\"text\": ['This is a Husky', 'This is a Golden Retriever', 'This is a German Sheperd', 'This is a Samoyed.']})\n",
    "print(\"Label probs:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e34f72",
   "metadata": {},
   "source": [
    "Clearly, according to the probabilities, we know there is a Husky in the photo (which I think is correct)!\n",
    "\n",
    "Let's try a harder example. Below is a photo of two Segways. This object class is not common in most existing vision datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0808ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://live.staticflickr.com/7236/7114602897_9cf00b2820_b.jpg\"\n",
    "segway_image = download(url)\n",
    "\n",
    "pil_img = Image(filename=segway_image)\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6556d",
   "metadata": {},
   "source": [
    "Given several text queries, CLIP can still predict the segway class correctly with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = predictor.predict_proba({\"image\": [segway_image]}, {\"text\": ['segway', 'bicycle', 'wheel', 'car']})\n",
    "print(\"Label probs:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a9fb4",
   "metadata": {},
   "source": [
    "This is amazing, right? Now a bit knowledge on why and how CLIP works. CLIP is called Contrastive Language-Image Pre-training. It is trained on a massive number of data (400M image-text pairs). By using a simple loss objective, CLIP tries to predict which out of a set of randomly sampled text is actually paired with an given image in the training dataset. As a result, CLIP models can then be applied to nearly arbitrary visual classification tasks just like the examples we have shown above.\n",
    "\n",
    "## More about CLIP\n",
    "\n",
    "CLIP is powerful, and it was designed to mitigate a number of major problems in the standard deep learning approach to computer vision, such as costly datasets, closed set prediction and poor generalization performance. CLIP is a good solution to many problems, however, it is not the ultimate solution. CLIP has its own limitations. For example, CLIP is vulnerable to typographic attacks, i.e., if you add some text to an image, CLIP's predictions will be easily affected by the text. Let's see one example from OpenAI's blog post on [multimodal neurons](https://openai.com/blog/multimodal-neurons/).\n",
    "\n",
    "Suppose we have a photo of a Granny Smith apple,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ba84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdn.openai.com/multimodal-neurons/assets/apple/apple-blank.jpg\"\n",
    "image_path = download(url)\n",
    "\n",
    "pil_img = Image(filename=image_path)\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac781bd",
   "metadata": {},
   "source": [
    "We then try to classify this image to several classes, such as Granny Smith, iPod, library, pizza, toaster and dough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a320f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = predictor.predict_proba({\"image\": [image_path]}, {\"text\": ['Granny Smith', 'iPod', 'library', 'pizza', 'toaster', 'dough']})\n",
    "print(\"Label probs:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee1ca9",
   "metadata": {},
   "source": [
    "We can see that zero-shot classification works great, it predicts apple with almost 100% confidence. But if we add a text to the apple like this,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337561ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg\"\n",
    "image_path = download(url)\n",
    "\n",
    "pil_img = Image(filename=image_path)\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea8ba97",
   "metadata": {},
   "source": [
    "Then we use the same class names to perform zero-shot classification,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75898e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = predictor.predict_proba({\"image\": [image_path]}, {\"text\": ['Granny Smith', 'iPod', 'library', 'pizza', 'toaster', 'dough']})\n",
    "print(\"Label probs:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540afa8",
   "metadata": {},
   "source": [
    "Suddenly, the apple becomes iPod.\n",
    "\n",
    "CLIP also has other limitations. If you are interested, you can read [CLIP paper](https://arxiv.org/abs/2103.00020) for more details. Or you can stay here, play with your own examples!\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/autogluon/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "## Customization\n",
    "\n",
    "To learn how to customize AutoMM, please refer to [Customize AutoMM](../advanced_topics/customization.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
