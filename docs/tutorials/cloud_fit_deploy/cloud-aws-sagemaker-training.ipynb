{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19aeec9",
   "metadata": {},
   "source": [
    "# Cloud Training with AWS SageMaker\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-sagemaker-training.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/master/docs/tutorials/cloud_fit_deploy/cloud-aws-sagemaker-training.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "To help with AutoGluon models training, AWS developed a set of training and inference [deep learning containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#autogluon-training-containers). \n",
    "The containers can be used to train models with CPU and GPU instances and deployed as a SageMaker endpoint or used as a batch transform job.\n",
    "\n",
    "The full end-to-end example is available in [amazon-sagemaker-examples](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/autogluon-tabular-containers) repository.\n",
    "\n",
    "## Pre-requisites\n",
    "Before starting ensure that the latest version of sagemaker python API is installed via (`pip install --upgrade sagemaker`). \n",
    "This is required to ensure the information about newly released containers is available.\n",
    "\n",
    "## Training Scripts\n",
    "\n",
    "To start using the containers, a user training script and the [wrapper classes](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/autogluon-tabular-containers/ag_model.py) are required.\n",
    "When authoring a training/inference [scripts](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/autogluon-tabular-containers/scripts/), \n",
    "please refer to SageMaker [documentation](https://sagemaker.readthedocs.io/en/stable/overview.html#prepare-a-training-script).\n",
    "\n",
    "Here is one of the possible `TabularPredictor` training scripts, which takes AutoGluon parameters as a YAML config and outputs predictions, models leaderboard and feature importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61348e6b",
   "metadata": {},
   "source": [
    "```python\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "import yaml\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "\n",
    "def get_input_path(path):\n",
    "    file = os.listdir(path)[0]\n",
    "    if len(os.listdir(path)) > 1:\n",
    "        print(f\"WARN: more than one file is found in {channel} directory\")\n",
    "    print(f\"Using {file}\")\n",
    "    filename = f\"{path}/{file}\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_env_if_present(name):\n",
    "    result = None\n",
    "    if name in os.environ:\n",
    "        result = os.environ[name]\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ------------------------------------------------------------ Arguments parsing\n",
    "    print(\"Starting AG\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\n",
    "        \"--output-data-dir\", type=str, default=get_env_if_present(\"SM_OUTPUT_DATA_DIR\")\n",
    "    )\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=get_env_if_present(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=get_env_if_present(\"SM_NUM_GPUS\"))\n",
    "    parser.add_argument(\"--train_dir\", type=str, default=get_env_if_present(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test_dir\", type=str, required=False, default=get_env_if_present(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--ag_config\", type=str, default=get_env_if_present(\"SM_CHANNEL_CONFIG\"))\n",
    "    parser.add_argument(\"--serving_script\", type=str, default=get_env_if_present(\"SM_CHANNEL_SERVING\"))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(f\"Args: {args}\")\n",
    "\n",
    "    # See SageMaker-specific environment variables: https://sagemaker.readthedocs.io/en/stable/overview.html#prepare-a-training-script\n",
    "    os.makedirs(args.output_data_dir, mode=0o777, exist_ok=True)\n",
    "\n",
    "    config_file = get_input_path(args.ag_config)\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.safe_load(f)  # AutoGluon-specific config\n",
    "\n",
    "    if args.n_gpus:\n",
    "        config[\"num_gpus\"] = int(args.n_gpus)\n",
    "\n",
    "    print(\"Running training job with the config:\")\n",
    "    pprint(config)\n",
    "\n",
    "    # ---------------------------------------------------------------- Training\n",
    "\n",
    "    train_file = get_input_path(args.train_dir)\n",
    "    train_data = TabularDataset(train_file)\n",
    "    \n",
    "    save_path = os.path.normpath(args.model_dir)\n",
    "\n",
    "    ag_predictor_args = config[\"ag_predictor_args\"]\n",
    "    ag_predictor_args[\"path\"] = save_path\n",
    "    ag_fit_args = config[\"ag_fit_args\"]\n",
    "\n",
    "    predictor = TabularPredictor(**ag_predictor_args).fit(train_data, **ag_fit_args)\n",
    "\n",
    "    # --------------------------------------------------------------- Inference\n",
    "\n",
    "    if args.test_dir:\n",
    "        test_file = get_input_path(args.test_dir)\n",
    "        test_data = TabularDataset(test_file)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred_proba = predictor.predict_proba(test_data)\n",
    "        if config.get(\"output_prediction_format\", \"csv\") == \"parquet\":\n",
    "            y_pred_proba.to_parquet(f\"{args.output_data_dir}/predictions.parquet\")\n",
    "        else:\n",
    "            y_pred_proba.to_csv(f\"{args.output_data_dir}/predictions.csv\")\n",
    "\n",
    "        # Leaderboard\n",
    "        if config.get(\"leaderboard\", False):\n",
    "            lb = predictor.leaderboard(test_data, silent=False)\n",
    "            lb.to_csv(f\"{args.output_data_dir}/leaderboard.csv\")\n",
    "\n",
    "        # Feature importance\n",
    "        if config.get(\"feature_importance\", False):\n",
    "            feature_importance = predictor.feature_importance(test_data)\n",
    "            feature_importance.to_csv(f\"{args.output_data_dir}/feature_importance.csv\")\n",
    "    else:\n",
    "        if config.get(\"leaderboard\", False):\n",
    "            lb = predictor.leaderboard(silent=False)\n",
    "            lb.to_csv(f\"{args.output_data_dir}/leaderboard.csv\")\n",
    "            \n",
    "    if args.serving_script:\n",
    "        print(\"Saving serving script\")\n",
    "        serving_script_saving_path = os.path.join(save_path, \"code\")\n",
    "        os.mkdir(serving_script_saving_path)\n",
    "        serving_script_path = get_input_path(args.serving_script)\n",
    "        shutil.move(\n",
    "            serving_script_path,\n",
    "            os.path.join(\n",
    "                serving_script_saving_path, os.path.basename(serving_script_path)\n",
    "            ),\n",
    "        )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562bebc",
   "metadata": {},
   "source": [
    "For training other types of AutoGluon Predictors, i.e. MultiModalPredictor, the training script you provided will be quite similar to the one above.\n",
    "Mostly, you just need to replace `TabularPredictor` to be `MultiModalPredictor` for example.\n",
    "Keep in mind that the specific Predictor type you want to train might not support the same feature sets as `TabularPredictor`.\n",
    "For example, `leaderboard` does not exist for all Predictors.\n",
    "\n",
    "\n",
    "### Notes for Training\n",
    "1. If your use case involves image modality, you will need to pass the images as a compressed file to the training container (similarly to how you would pass in train data), decompress the file in the training container, and update the training data columns with the updated image path in the container.\n",
    "\n",
    "2. If you wish to deploy or do batch inference on the trained TextPredictor/MultiModalPredictor on sagemaker later, you will need to save the model with `standalone` flag, which avoids internet access to load the model later.\n",
    "For example, `predictor.save(path='MY_PATH', standalone=True)`.\n",
    "SageMaker container is known to have issue connecting to HuggingFace. That's why we need to save the artifacts in offline mode.\n",
    "\n",
    "Tabular example YAML config:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58895650",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# AutoGluon Predictor constructor arguments\n",
    "# - see https://github.com/autogluon/autogluon/blob/v0.5.2/tabular/src/autogluon/tabular/predictor/predictor.py#L56-L181\n",
    "ag_predictor_args:\n",
    "  eval_metric: roc_auc\n",
    "  label: class\n",
    "\n",
    "# AutoGluon Predictor.fit arguments\n",
    "# - see https://github.com/autogluon/autogluon/blob/v0.5.2/tabular/src/autogluon/tabular/predictor/predictor.py#L286-L711\n",
    "ag_fit_args:\n",
    "  presets: \"medium_quality_faster_train\"\n",
    "  num_bag_folds: 2\n",
    "  num_bag_sets: 1\n",
    "  num_stack_levels: 0\n",
    "\n",
    "output_prediction_format: csv  # predictions output format: csv or parquet\n",
    "feature_importance: true       # calculate and save feature importance if true\n",
    "leaderboard: true              # save leaderboard output if true\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40821c",
   "metadata": {},
   "source": [
    "Another example, MultiModal example YAML config:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff521b49",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# AutoGluon Predictor constructor arguments\n",
    "# - see https://github.com/autogluon/autogluon/blob/v0.5.2/multimodal/src/autogluon/multimodal/predictor.py#L123-L180\n",
    "ag_predictor_args:\n",
    "  eval_metric: acc\n",
    "  label: label\n",
    "\n",
    "# AutoGluon Predictor.fit arguments\n",
    "# - see https://github.com/autogluon/autogluon/blob/v0.5.2/multimodal/src/autogluon/multimodal/predictor.py#L246-L363\n",
    "ag_fit_args:\n",
    "  presets: \"high_quality\"\n",
    "  time_limit: 120\n",
    "\n",
    "output_prediction_format: csv  # predictions output format: csv or parquet\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466d1d9",
   "metadata": {},
   "source": [
    "Other predictors would follow similar format as the previous two examples.\n",
    "\n",
    "## Training\n",
    "\n",
    "Note the `ag_model` imports are sourced from [this helper package](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/autogluon-tabular-containers/ag_model.py).\n",
    "\n",
    "To train AutoGluon model, set up a SageMaker session:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f404d5",
   "metadata": {},
   "source": [
    "```python\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "# Helper wrappers referred earlier\n",
    "from ag_model import (\n",
    "    AutoGluonSagemakerEstimator,\n",
    "    AutoGluonNonRepackInferenceModel,\n",
    "    AutoGluonSagemakerInferenceModel,\n",
    "    AutoGluonRealtimePredictor,\n",
    "    AutoGluonBatchPredictor,\n",
    ")\n",
    "from sagemaker import utils\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = f\"autogluon_sm/{utils.sagemaker_timestamp()}\"\n",
    "output_path = f\"s3://{bucket}/{s3_prefix}/output/\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7a374",
   "metadata": {},
   "source": [
    "Create a training task:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda21c1",
   "metadata": {},
   "source": [
    "```python\n",
    "ag = AutoGluonSagemakerEstimator(\n",
    "    role=role,\n",
    "    entry_point=\"YOUR_TRAINING_SCRIPT_PATH\",\n",
    "    region=region,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",  # You might want to use GPU instances for Text/Image/MultiModal Predictors etc\n",
    "    framework_version=\"0.6\",  # Replace this with the AutoGLuon DLC container version you want to use\n",
    "    py_version=\"py38\",\n",
    "    base_job_name=\"YOUR_JOB_NAME\",\n",
    "    # Disable torch profiler instrumentation to avoid deserialization issues during deployment\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46fe8d",
   "metadata": {},
   "source": [
    "Upload the required inputs, via SageMaker session (in this case it is a training set, test set and training YAML config) and start the training job.\n",
    "Please read more on \"Why do I see a repack step in my SageMaker pipeline?\" [here](https://docs.aws.amazon.com/sagemaker/latest/dg/mlopsfaq.html).\n",
    "Example of inference script can be found here: [tabular_serve.py](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/autogluon-tabular-containers/scripts/tabular_serve.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5c471",
   "metadata": {},
   "source": [
    "```python\n",
    "s3_prefix = f\"autogluon_sm/{utils.sagemaker_timestamp()}\"\n",
    "train_input = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"data\", \"train.csv\"), key_prefix=s3_prefix\n",
    ")\n",
    "eval_input = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"data\", \"test.csv\"), key_prefix=s3_prefix\n",
    ")\n",
    "config_input = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"config\", \"config-med.yaml\"), key_prefix=s3_prefix\n",
    ")\n",
    "inference_script = ag.sagemaker_session.upload_data(\n",
    "    path=os.path.join(\"scripts\", \"INFERENCE_SCRIPT_LOCATION\"), key_prefix=s3_prefix\n",
    ")\n",
    "\n",
    "job_name = utils.unique_name_from_base(\"test-autogluon-image\")\n",
    "ag.fit(\n",
    "    {\n",
    "        \"config\": config_input,\n",
    "        \"train\": train_input,\n",
    "        \"test\": eval_input,\n",
    "        \"serving\": inference_script,\n",
    "    },\n",
    "    job_name=job_name,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b20780",
   "metadata": {},
   "source": [
    "Once the models are trained, they will be available in S3 location specified in `ag.model_data` field. The model is fully portable and can be downloaded locally\n",
    "if needed.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial we explored how to train AutoGluon models using SageMaker. Learn how to deploy the trained models using \n",
    "AWS SageMaker - [Deploying AutoGluon Models with AWS SageMaker](cloud-aws-sagemaker-deployment.ipynb) or AWS Lambda - [Deploying AutoGluon models with serverless templates](cloud-aws-lambda-deployment.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
