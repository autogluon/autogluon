{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd6985f",
   "metadata": {},
   "source": [
    "# Deploying AutoGluon Models with AWS SageMaker\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/new/docs/tutorials/cloud_fit_deploy/cloud-aws-sagemaker-deployment.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/new/docs/tutorials/cloud_fit_deploy/cloud-aws-sagemaker-deployment.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "After learning how to train a model using AWS SageMaker [Cloud Training with AWS SageMaker](cloud-aws-sagemaker-training.ipynb), in this section we will learn how to deploy \n",
    "trained models using AWS SageMaker and Deep Learning Containers. \n",
    "\n",
    "The full end-to-end example is available in [amazon-sagemaker-examples](https://github.com/aws/amazon-sagemaker-examples/tree/master/advanced_functionality/autogluon-tabular-containers) repository.\n",
    "\n",
    "## Pre-requisites\n",
    "Before starting ensure that the latest version of sagemaker python API is installed via (`pip install --upgrade sagemaker`). \n",
    "This is required to ensure the information about newly released containers is available.\n",
    "\n",
    "## Endpoint Deployment - Inference Script\n",
    "\n",
    "To start using the containers, an inference script and the [wrapper classes](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/autogluon-tabular-containers/ag_model.py) are required.\n",
    "When authoring an inference [scripts](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/autogluon-tabular-containers/scripts/), \n",
    "please refer to SageMaker [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html).\n",
    "\n",
    "Here is one of the possible inference scripts. \n",
    "\n",
    "- the `model_fn` function is responsible for loading your model. It takes a `model_dir` argument that specifies where the model is stored. \n",
    "\n",
    "- the `transform_fn` function is responsible for deserializing your input data so that it can be passed to your model. It takes input data and \n",
    "content type as parameters, and returns deserialized data. The SageMaker inference toolkit provides a default implementation that deserializes \n",
    "the following content types: JSON, CSV, numpy array, NPZ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45540e43",
   "metadata": {},
   "source": [
    "```python\n",
    "from autogluon.tabular import TabularPredictor\n",
    "# or from autogluon.multimodal import MultiModalPredictor for example\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from autogluon.core.constants import REGRESSION\n",
    "from autogluon.core.utils import get_pred_from_proba_df\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"loads model from previously saved artifact\"\"\"\n",
    "    model = TabularPredictor.load(model_dir)  # or model = MultiModalPredictor.load(model_dir) for example\n",
    "    globals()[\"column_names\"] = model.feature_metadata_in.get_features()\n",
    "    return model\n",
    "\n",
    "\n",
    "def transform_fn(\n",
    "    model, request_body, input_content_type, output_content_type=\"application/json\"\n",
    "):\n",
    "    if input_content_type == \"application/x-parquet\":\n",
    "        buf = BytesIO(request_body)\n",
    "        data = pd.read_parquet(buf)\n",
    "\n",
    "    elif input_content_type == \"text/csv\":\n",
    "        buf = StringIO(request_body)\n",
    "        data = pd.read_csv(buf)\n",
    "\n",
    "    elif input_content_type == \"application/json\":\n",
    "        buf = StringIO(request_body)\n",
    "        data = pd.read_json(buf)\n",
    "\n",
    "    elif input_content_type == \"application/jsonl\":\n",
    "        buf = StringIO(request_body)\n",
    "        data = pd.read_json(buf, orient=\"records\", lines=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"{input_content_type} input content type not supported.\")\n",
    "\n",
    "    if model.problem_type != REGRESSION:\n",
    "        pred_proba = model.predict_proba(data, as_pandas=True)\n",
    "        pred = get_pred_from_proba_df(pred_proba, problem_type=model.problem_type)\n",
    "        pred_proba.columns = [str(c) + \"_proba\" for c in pred_proba.columns]\n",
    "        pred.name = str(pred.name) + \"_pred\" if pred.name is not None else \"pred\"\n",
    "        prediction = pd.concat([pred, pred_proba], axis=1)\n",
    "    else:\n",
    "        prediction = model.predict(data, as_pandas=True)\n",
    "    if isinstance(prediction, pd.Series):\n",
    "        prediction = prediction.to_frame()\n",
    "\n",
    "    if \"application/x-parquet\" in output_content_type:\n",
    "        prediction.columns = prediction.columns.astype(str)\n",
    "        output = prediction.to_parquet()\n",
    "        output_content_type = \"application/x-parquet\"\n",
    "    elif \"application/json\" in output_content_type:\n",
    "        output = prediction.to_json()\n",
    "        output_content_type = \"application/json\"\n",
    "    elif \"text/csv\" in output_content_type:\n",
    "        output = prediction.to_csv(index=None)\n",
    "        output_content_type = \"text/csv\"\n",
    "    else:\n",
    "        raise ValueError(f\"{output_content_type} content type not supported\")\n",
    "\n",
    "    return output, output_content_type\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522de316",
   "metadata": {},
   "source": [
    "For inference with other types of AutoGluon Predictors, i.e. TextPredictor, the inference script you provided will be quite similar to the one above.\n",
    "Mostly, you just need to replace `TabularPredictor` to be `TextPredictor` for example.\n",
    "\n",
    "### Note on image modality\n",
    "To do inference on image modality, you would need to embed the image info, as bytes for example, into a column of the test data.\n",
    "Then in the inference container, if you are using the `MultiModalPredictor`, you just need to decode the aforementioned image column and feed the test data to it.\n",
    "\n",
    "For example, to encode the image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5cdcd",
   "metadata": {},
   "source": [
    "```python\n",
    "def read_image_bytes_and_encode(image_path):\n",
    "    image_obj = open(image_path, 'rb')\n",
    "    image_bytes = image_obj.read()\n",
    "    image_obj.close()\n",
    "    b85_image = base64.b85encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "    return b85_image\n",
    "\n",
    "\n",
    "def convert_image_path_to_encoded_bytes_in_dataframe(dataframe, image_column):\n",
    "    assert image_column in dataframe, 'Please specify a valid image column name'\n",
    "    dataframe[image_column] = [read_image_bytes_and_encode(path) for path in dataframe[image_column]]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "test_data_image_column = \"YOUR_COLUMN_CONTAINING_IMAGE_PATH\"\n",
    "test_data = convert_image_path_to_encoded_bytes_in_dataframe(test_data, test_data_image_column)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7fd8e0",
   "metadata": {},
   "source": [
    "For example, to decode the image the inference container:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc48851",
   "metadata": {},
   "source": [
    "```python\n",
    "test_data[image_column] = [base64.b85decode(bytes) for bytes in test_data[image_column]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdf56b",
   "metadata": {},
   "source": [
    "Note that if you are using the `TabularPredictor`, you would need to store the image into the disk and update the test data with the image paths accordingly.\n",
    "\n",
    "For example, to decode the image and save to disk in the inference container:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2201f",
   "metadata": {},
   "source": [
    "```python\n",
    "image_index = 0\n",
    "\n",
    "\n",
    "def _save_image_and_update_dataframe_column(bytes):\n",
    "    global image_index\n",
    "    im = Image.open(BytesIO(base64.b85decode(bytes)))\n",
    "    im_name = f'Image_{image_index}.png'\n",
    "    im.save(im_name)\n",
    "    image_index += 1\n",
    "\n",
    "    return im_name\n",
    "\n",
    "\n",
    "test_data[image_column] = [_save_image_and_update_dataframe_column(bytes) for bytes in test_data[image_column]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e2056",
   "metadata": {},
   "source": [
    "## Deployment as an inference endpoint\n",
    "\n",
    "To deploy AutoGluon model as a SageMaker inference endpoint, we configure SageMaker session first:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5636bba7",
   "metadata": {},
   "source": [
    "```python\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "# Helper wrappers referred earlier\n",
    "from ag_model import (\n",
    "    AutoGluonSagemakerEstimator,\n",
    "    AutoGluonNonRepackInferenceModel,\n",
    "    AutoGluonSagemakerInferenceModel,\n",
    "    AutoGluonRealtimePredictor,\n",
    "    AutoGluonBatchPredictor,\n",
    ")\n",
    "from sagemaker import utils\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = f\"autogluon_sm/{utils.sagemaker_timestamp()}\"\n",
    "output_path = f\"s3://{bucket}/{s3_prefix}/output/\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db03187",
   "metadata": {},
   "source": [
    "Upload the model archive trained earlier (if you trained AutoGluon model locally, it must be a zip archive of the model output directory).\n",
    "Remember, you would need to save the model artifacts with `standalone=True` if the model artifact is of type `TextPredictor`/`MultiModalPredictor`.\n",
    "Otherwise, you will have trouble loading the model in the container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a83e3",
   "metadata": {},
   "source": [
    "```python\n",
    "endpoint_name = sagemaker.utils.unique_name_from_base(\"sagemaker-autogluon-serving-trained-model\")\n",
    "\n",
    "model_data = sagemaker_session.upload_data(\n",
    "    path=os.path.join(\".\", \"model.tar.gz\"), key_prefix=f\"{endpoint_name}/models\"\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34d88b",
   "metadata": {},
   "source": [
    "Deploy the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8d0406",
   "metadata": {},
   "source": [
    "```python\n",
    "instance_type = \"ml.m5.2xlarge\"  # You might want to use GPU instances, i.e. ml.g4dn.2xlarge for Text/Image/MultiModal Predictors etc\n",
    "\n",
    "model = AutoGluonNonRepackInferenceModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    region=region,\n",
    "    framework_version=\"0.6\",  # Replace this with the AutoGluon DLC container version you want to use\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"YOUR_SERVING_SCRIPT_PATH\",  # example: \"tabular_serve.py\"\n",
    ")\n",
    "\n",
    "model.deploy(initial_instance_count=1, serializer=CSVSerializer(), instance_type=instance_type)\n",
    "predictor = AutoGluonRealtimePredictor(model.endpoint_name)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefba23",
   "metadata": {},
   "source": [
    "Once the predictor is deployed, it can be used for inference in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9997f",
   "metadata": {},
   "source": [
    "```python\n",
    "predictions = predictor.predict(data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe3041",
   "metadata": {},
   "source": [
    "## Using SageMaker batch transform for offline processing\n",
    "\n",
    "Deploying a trained model to a hosted endpoint has been available in SageMaker since launch and is a great way to provide real-time \n",
    "predictions to a service like a website or mobile app. But, if the goal is to generate predictions from a trained model on a large \n",
    "dataset where minimizing latency isnâ€™t a concern, then the batch transform functionality may be easier, more scalable, and more appropriate.\n",
    "\n",
    "[Read more about Batch Transform.](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)\n",
    "\n",
    "Upload the model archive trained earlier (if you trained AutoGluon model locally, it must be a zip archive of the model output directory).\n",
    "Remember, you would need to save the model artifacts with `standalone=True` if the model artifact is of type `TextPredictor`/`MultiModalPredictor`.\n",
    "Otherwise, you will have trouble loading the model in the container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88077869",
   "metadata": {},
   "source": [
    "```python\n",
    "endpoint_name = sagemaker.utils.unique_name_from_base(\n",
    "    \"sagemaker-autogluon-batch_transform-trained-model\"\n",
    ")\n",
    "\n",
    "model_data = sagemaker_session.upload_data(\n",
    "    path=os.path.join(\".\", \"model.tar.gz\"), key_prefix=f\"{endpoint_name}/models\"\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a63be0",
   "metadata": {},
   "source": [
    "Prepare transform job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9183ad3",
   "metadata": {},
   "source": [
    "```python\n",
    "instance_type = \"ml.m5.2xlarge\"  # You might want to use GPU instances, i.e. ml.g4dn.2xlarge for Text/Image/MultiModal Predictors etc\n",
    "\n",
    "model = AutoGluonSagemakerInferenceModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    region=region,\n",
    "    framework_version=\"0.6\",  # Replace this with the AutoGluon DLC container version you want to use\n",
    "    py_version=\"py38\",\n",
    "    instance_type=instance_type,\n",
    "    entry_point=\"YOUR_BATCH_SERVE_SCRIPT\",  # example: \"tabular_serve.py\"\n",
    "    source_dir=\"scripts\",\n",
    "    predictor_cls=AutoGluonBatchPredictor,\n",
    "    # or AutoGluonMultiModalPredictor if model is trained by MultiModalPredictor.\n",
    "    # Please refer to https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/autogluon-tabular-containers/ag_model.py#L60-L64 on how you would customize it.\n",
    ")\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    strategy=\"MultiRecord\",\n",
    "    max_payload=6,\n",
    "    max_concurrent_transforms=1,\n",
    "    output_path=output_path,\n",
    "    accept=\"application/json\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93addc2b",
   "metadata": {},
   "source": [
    "Upload the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e04220",
   "metadata": {},
   "source": [
    "```python\n",
    "test_input = transformer.sagemaker_session.upload_data(path=os.path.join(\"data\", \"test.csv\"), key_prefix=s3_prefix)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5425b01",
   "metadata": {},
   "source": [
    "The inference script would be identical to the one used for deployment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251067f0",
   "metadata": {},
   "source": [
    "```python\n",
    "# or from autogluon.multimodal import MultiModalPredictor for example\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"loads model from previously saved artifact\"\"\"\n",
    "    model = TabularPredictor.load(model_dir)  # or model = MultiModalPredictor.load(model_dir) for example\n",
    "    model.persist_models()  # This line only works for TabularPredictor\n",
    "    globals()[\"column_names\"] = model.feature_metadata_in.get_features()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def transform_fn(model, request_body, input_content_type, output_content_type=\"application/json\"):\n",
    "\n",
    "    if input_content_type == \"text/csv\":\n",
    "        buf = StringIO(request_body)\n",
    "        data = pd.read_csv(buf, header=None)\n",
    "        num_cols = len(data.columns)\n",
    "        if num_cols != len(column_names):\n",
    "            raise Exception(f\"Invalid data format. Input data has {num_cols} while the model expects {len(column_names)}\")\n",
    "        else:\n",
    "            data.columns = column_names\n",
    "    else:\n",
    "        raise Exception(f\"{input_content_type} content type not supported\")\n",
    "\n",
    "    pred = model.predict(data)\n",
    "    pred_proba = model.predict_proba(data)\n",
    "    prediction = pd.concat([pred, pred_proba], axis=1)\n",
    "\n",
    "    return prediction.to_json(), output_content_type\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f6837",
   "metadata": {},
   "source": [
    "Run the transform job.\n",
    "\n",
    "When making predictions on a large dataset, you can exclude attributes that aren't needed for prediction. After the predictions have been made, you can \n",
    "associate some of the excluded attributes with those predictions or with other input data in your report. By using batch transform to perform these data \n",
    "processing steps, you can often eliminate additional preprocessing or postprocessing. You can use input files in JSON and CSV format only. \n",
    "More details on how to use filters are available here: [Associate Prediction Results with Input Records](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html).\n",
    "In this specific case we will use `input_filter` argument to get first 14 columns, thus removing target variable from the test set and `output_filter` to\n",
    "extract only the actual classes predictions without scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98973863",
   "metadata": {},
   "source": [
    "```python\n",
    "transformer.transform(\n",
    "    test_input,\n",
    "    input_filter=\"$[:14]\",  # filter-out target variable\n",
    "    split_type=\"Line\",\n",
    "    content_type=\"text/csv\",\n",
    "    output_filter=\"$['class']\",  # keep only prediction class in the output\n",
    ")\n",
    "\n",
    "transformer.wait()\n",
    "\n",
    "output_s3_location = f\"{transformer.output_path[:-1]}/{output_file_name}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9727590",
   "metadata": {},
   "source": [
    "The output file will be in `output_s3_location` variable.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial we explored a few options how to deploy AutoGluon models using SageMaker. To explore more, refer to \n",
    "[SageMaker inference](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) documentation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}