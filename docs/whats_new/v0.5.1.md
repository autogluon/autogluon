# Version 0.5.1

# Changes

## AutoMM

Changed to a new namespace `autogluon.multimodal` (AutoMM), which is a deep learning "model zoo" of model zoos. On one hand, AutoMM can automatically train deep models for unimodal (image-only, text-only or tabular-only) problems. On the other hand, AutoMM can automatically solve multimodal (any combinations of image, text, and tabular) problems by fusing multiple deep learning models. In addition, AutoMM can be used as a base model in AutoGluon Tabular and participate in the model ensemble.

### New features

- Supported zero-shot learning with CLIP (#1922) @zhiqiangdon
  - Users can directly perform zero-shot image classification with the [CLIP model](https://arxiv.org/abs/2103.00020). Moreover, users can extract image and text embeddings with CLIP to do image-to-text or text-to-image retrieval. 

- Improved efficient finetuning
  - Support “bit_fit”, “norm_fit“, “lora”, “lora_bias”, “lora_norm”. In four multilingual datasets ([xnli](https://huggingface.co/datasets/xnli), [stsb_multi_mt](http://stsb_multi_mt/), [paws-x](https://huggingface.co/datasets/paws-x), [amazon_reviews_multi](https://huggingface.co/datasets/amazon_reviews_multi)), “lora_bias”, which is a combination of [LoRA](https://arxiv.org/abs/2106.09685) and [BitFit](https://arxiv.org/abs/2106.10199), achieved the best overall performance. Compared to finetuning the whole network, “lora_bias” will only finetune **<0.5%** of the network parameters and can achieve comparable performance on “stsb_multi_mt” (#1780, #1809). @Raldir @zhiqiangdon
  - Support finetuning the [mT5-XL](https://huggingface.co/google/mt5-xl) model that has 1.7B parameters on a single NVIDIA G4 GPU. In AutoMM, we only use the T5-encoder (1.7B parameters) like [Sentence-T5](https://aclanthology.org/2022.findings-acl.146.pdf). (#1933) @sxjscience

- Added more data augmentation techniques
  - [Mixup](https://arxiv.org/pdf/1710.09412.pdf) for image data. (#1730) @Linuxdex
  - [TrivialAugment](https://arxiv.org/pdf/2103.10158.pdf) for both image and text data. (#1792) @lzcemma
  - [Easy text augmentations](https://arxiv.org/pdf/1901.11196.pdf). (#1756) @lzcemma

- Enhanced teacher-student model distillation
  - Support distilling the knowledge from a unimodal/multimodal teacher model to a student model. (#1670, #1895) @zhiqiangdon

### More tutorials and examples

- [Beginner tutorials](https://auto.gluon.ai/stable/tutorials/multimodal/index.html) of applying AutoMM to image, text, or multimodal (including tabular) data. (#1861, #1908, #1858, #1869) @bryanyzhu @sxjscience @zhiqiangdon

- [A zero-shot image classification tutorial](https://auto.gluon.ai/stable/tutorials/multimodal/clip_zeroshot.html) with the CLIP model. (#1942) @bryanyzhu

- A tutorial of using [CLIP model to extract embeddings](https://auto.gluon.ai/stable/tutorials/multimodal/clip_embedding.html) for image-text retrieval. (#1957) @bryanyzhu

- [A tutorial](https://auto.gluon.ai/stable/tutorials/multimodal/customization.html) to introduce comprehensive AutoMM configurations (#1861). @zhiqiangdon

- [AutoMM for tabular data examples](https://github.com/awslabs/autogluon/tree/master/examples/automm/tabular_dl) (#1752, #1893, #1903). @yiqings

- [AutoMM distillation example](https://github.com/awslabs/autogluon/tree/master/examples/automm/distillation) (#1846). @FANGAreNotGnu

- A Kaggle notebook about how to use AutoMM to predict pet adoption: https://www.kaggle.com/code/linuxdex/use-autogluon-to-predict-pet-adoption. The model achieves the score equivalent to **top 1% (20th/3537) in this kernel-only competition (test data is only available in the kernel without internet access)** (#1796, #1847, #1894, #1943). @Linuxdex


## TimeSeries

We are happy to announce AutoGluon-TimeSeries! Starting with v0.5, AutoGluon now supports AutoML for time series forecasting, 
leveraging both statistical forecasting methods such as ETS and ARIMA, as well as modern deep learning architectures
through [GluonTS](https://ts.gluon.ai/stable/). The new module also features a weighted ensemble of time series models, 
and is geared towards probabilistic (quantile) forecasting to enable many use cases from demand and supply chain forecasting 
to financial applications.

All time series forecasting tasks are supported via the familiar AutoGluon interface, through the
`TimeSeriesPredictor` class. Start forecasting today with the AutoGluon-TimeSeries 
[quick start guide](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-quickstart.html).

Contributor List: @canerturkmen, @huibinshen, @Innixma, @yinweisu, @shchur, @gradientsky  

### Fixes and enhancements in v0.5.1

- Add early stopping for AutoGluon-TimeSeries models (#1917) @huibinshen
- Allow for automatically inferring seasonality period from `TimeSeriesDataFrame` index in `AutoETS`, intelligently setting seasonality to be used in ETS models by default. (#1914) @canerturkmen, @shchur
- Changes in model presets, enabling `ARIMA` and GluonTS's `Transformer` models to be enabled by default and removing `MQCNN` models (#1914). @canerturkmen, @shchur
- Fix for an issue that affected data sets with custom target column names when using `TimeSeriesPredictor` (#1901) @canerturkmen
- Capping `gluonts`, `sktime` versions (#1914, #1916) @yinweisu, @canerturkmen, @shchur
