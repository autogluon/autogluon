# Version 1.4.0

We are happy to announce the AutoGluon 1.4.0 release!

AutoGluon 1.4.0 introduces massive new features and improvements to both tabular and time series modules. In particular, we introduce the `extreme` preset to TabularPredictor, which sets a new state-of-the-art for predictive performance **by a massive margin** on datasets with fewer than 30000 samples.

This release contains [74 commits from 15 contributors](https://github.com/autogluon/autogluon/graphs/contributors?from=5%2F21%2F2025&to=7%2F26%2F2025&type=c)! See the full commit change-log here: https://github.com/autogluon/autogluon/compare/1.3.1...1.4.0

Join the community: [![](https://img.shields.io/discord/1043248669505368144?logo=discord&style=flat)](https://discord.gg/wjUmjqAc2N)
Get the latest updates: [![Twitter](https://img.shields.io/twitter/follow/autogluon?style=social)](https://twitter.com/autogluon)

This release supports Python versions 3.9, 3.10, 3.11, and 3.12. Loading models trained on older versions of AutoGluon is not supported. Please re-train models using AutoGluon 1.4.0.

--------

## Spotlight

### AutoGluon Tabular Extreme Preset

### AutoGluon Assistant (MLZero)
> *Multi-Agent System Powered by LLMs for End-to-end Multimodal ML Automation*

@FANGAreNotGnu
@HuawenShen
@boranhan

We are excited to present the [AutoGluon Assistant](https://github.com/autogluon/autogluon-assistant) 1.0 release. Level up from v0.1: v1.0 expands beyond tabular data to robustly support any and many modalities, including **image, text, tabular, audio and mixed-data pipelines**. This aligns precisely with the MLZero vision of comprehensive, modality-agnostic ML automation.

AutoGluon Assistant v1.0 is now synonymous with **"MLZero: A Multi-Agent System for End-to-end Machine Learning Automation"** ([arXiv:2505.13941](https://arxiv.org/abs/2505.13941)), the end-to-end, zero-human-intervention AutoML agent framework for multimodal data. Built on a novel **multi-agent architecture** using LLMs, MLZero handles perception, memory (semantic & episodic), code generation, execution, and iterative debugging â€” seamlessly transforming raw multimodal inputs into high-quality ML/DL pipelines.

- **No-code**: Users define tasks purely through natural language ("classify images of cats vs dogs with custom labels"), and MLZero delivers complete solutions with zero manual configuration or technical expertise required.
- **Built on proven foundations**: MLZero generates code using established, high-performance ML libraries rather than reinventing the wheel, ensuring robust solutions while maintaining the flexibility to easily integrate new libraries as they emerge.
- **Research-grade performance**: MLZero is extensively validated across 25 challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\%) and an average rank of 2.42. 
  
<div style="margin-left: auto;
            margin-right: auto;
            width: 30%">

| Dataset     | Ours | Codex CLI | Codex CLI (+reasoning) | AIDE | DS-Agent | AK |
|-------------|--------------------------|---------------|---------------|----------|--------------|--------|
| **Avg. Rank â†“** | **2.42** | 8.04 | 5.76 | 6.16 | 8.26 | 8.28 | 
| **Rel. Time â†“** | 1.0  | 0.15 | 0.23 | 2.83 | N/A  | 4.82 | 
| **Success â†‘**   | **92.0%** | 14.7% | 69.3% | 25.3% | 13.3% | 14.7% | 
</div>

- **Modular and extensible architecture**: We separate the design and implementation of each agent and prompts for different purposes, with a centralized manager coordinating them. This makes adding or editing agents, prompts, and workflows straightforward and intuitive for future development.

Weâ€™re also excited to introduce the newly redesigned **WebUI** in v1.0, now with a streamlined chatbot-style interface that makes interacting with MLZero intuitive and engaging. Furthermore, weâ€™re also bringing **MCP (Model Control Protocol)** integration to MLZero, enabling seamless remote orchestration of AutoML pipelines through a standardized protocolã€‚

AutoGluon Assistant is supported on Python 3.8 - 3.11 and is available on Linux.

Installation:
```bash
pip install uv
uv pip install autogluon.assistant>=1.0
```

To use CLI:
```bash
mlzero -i <input_data_dir>
```

To use webUI:
```bash
mlzero-backend   # command to start backend
mlzero-frontend  # command to start frontend on 8509 (default)
```

To use MCP:
```bash
# server
mlzero-backend # command to start backend
bash ./src/autogluon/mcp/server/start_services.sh # This will start the serviceâ€”run it in a new terminal.
# client
python ./src/autogluon/mcp/client/server.py
```

### Mitra

[@xiyuanzh](https://xiyuanzh.github.io/) @tonyhoo @Innixma [@dcmaddix](https://dcmaddix.github.io/)

ðŸš€ [Mitra](https://huggingface.co/autogluon/mitra-classifier) is a new state-of-the-art tabular foundation model developed by the AutoGluon team, natively supported in AutoGluon with just **three lines of code** via `predictor.fit(<mitra parameters>)`. Built on the in-context learning paradigm and **pretrained exclusively on synthetic data**, Mitra introduces a principled pretraining approach by carefully selecting and mixing diverse synthetic priors to promote robust generalization across a wide range of real-world tabular datasets.

ðŸ“Š Mitra achieves **state-of-the-art performance** on major benchmarks including TabRepo, TabZilla, AMLB, and TabArena, especially excelling on small tabular datasets with fewer than 5,000 samples and 100 features, for both **classification** and **regression** tasks.

ðŸ§  Mitra supports both **zero-shot** and **fine-tuning** modes and runs seamlessly on both **GPU** and **CPU**. Its weights are fully open-sourced under the Apache-2.0 license, making it a privacy-conscious and production-ready solution for enterprises concerned about data sharing and hosting.

ðŸ”— Learn more on HuggingFace:

* Classification model: [autogluon/mitra-classifier](https://huggingface.co/autogluon/mitra-classifier)
* Regression model: [autogluon/mitra-regressor](https://huggingface.co/autogluon/mitra-regressor)

We welcome community feedback for future iterations. Give us a like on HuggingFace if you want to see more cutting-edge foundation models for structured data!


### TabArena

@Innixma

--------

## General
- Support Apple Silicon and log it in the system info. [@tonyhoo] #5141
- Add load pickle from url support, fix save_str if root path. [@Innixma] #5142
- Add ag.ens. shortcut for ag_args_ensemble. [@Innixma] #5143
- Use pyarrow by default, remove fastparquet. [@Innixma] #5150
- Resolve AttributeError in LinearModel when using RAPIDS cuML models. [@tonyhoo] #5157
- add kwargs option to upload_file. [@Innixma] #5161
- prioritize the CUDA libraries from PyTorch wheel instead of the system/DLC. [@FireballDWF] #5163
- update numpy cap, thus 2.3.0 is allowed. [@FireballDWF] #5170
- Replace pkg_resources.parse_version with packaging.version.parse. [@shchur] #5182
- Update pandas, scikit-learn, and scipy version caps in setup utils. [@tonyhoo] #5194
- Add CPU utility functions for better CPU detection in restrained env such as docker and slurm cluster. [@tonyhoo] #5197
- Enhance spunge_augment and munge_augment functions for model distillation. [@tonyhoo] #5208
- Use joblib instead of loky for cpu detection. [@shchur] #5215
- Spunge Augmentation Speed-Up. [@mwhol] #5217
- Increase pytorch cap to 2.8 to enable 2.7. [@FireballDWF] #5089
- Resolve datetime deprecation warnings. [@emmanuel-ferdman] #5069


--------

## Tabular
- Add support for max_rows, max_features, max_classes, problem_types. [@Innixma] (#5181)
- Add `extreme` preset with meta-learned TabArena portfolio. [@Innixma] (#5211)
- Add RealMLP Model. [@Innixma] (#5190)
- Add TabPFNv2 Model. [@Innixma] (#5191)
- Add TabICL Model. [@Innixma] (#5193)
- Add Mitra Model. [@xiyuanzh], [@Innixma], [@tonyhoo] (#5195, #5218, #5232, #5221)
- Add TabM Model. [@Innixma] (#5196)
- Fix CatBoost crashing for problem_type="quantile" if len(quantile_levels) == 1. [@shchur] (#5201)
- Respect num_cpus/num_gpus in sequential_local fit. [@Innixma] (#5203)
- Switch to loky for get_cpu_count in all places. [@Innixma] (#5204)
- Add tabular foundational model cache from s3 to benchmark to avoid rate limit issue from HF. [@tonyhoo] (#5214)
- Fix default loss_function for CatBoostModel with problem_type='regression'. [@shchur] (#5216)
- Remove fobj in SOFTCLASS. [@rsj123] (#5219)
- Minor enhancements and fixes. [@adibiasio], [@Innixma] (#5158)
--------

## TimeSeries

### Highlights

- Major [efficiency improvements](https://github.com/autogluon/autogluon/pull/5159) to the core `TimeSeriesDataFrame` methods, resulting in up to 7x lower end-to-end `predictor.fit()` and `predict()` time when working with large datasets (>10M rows).

- New tabular forecasting model [`PerStepTabular`](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-model-zoo.html#autogluon.timeseries.models.PerStepTabularModel) that fits a separate tabular regression model for each time step in the forecast horizon. Both fitting and inference for the model are parallelized across cores, resulting in one of the most efficient and accurate implementations of this model among open-source Python packages.


### API Changes and Deprecations
- `DirectTabular` and `RecursiveTabular` models: hyperparameters `tabular_hyperparameters` and `tabular_fit_kwargs` are now deprecated in favor of `model_name` and `model_hyperparameters`.

    These models now fit a single regression model from `autogluon.tabular` under the hood instead of creating an entire `TabularPredictor`. This results in lower disk usage and API better aligned with the rest of the `timeseries` module.

    <details>
    <summary>Details and example usage</summary>

    ```python
    # New API: >= v1.4.0
    predictor.fit(
        ...,
        hyperparameters={
            "RecursiveTabular": {"model_name": "CAT", "model_hyperparameters": {"iterations": 100}}
        }
    )
    # Old API: <= v1.3.1
    predictor.fit(
        ...,
        hyperparameters={
            "RecursiveTabular": {"tabular_hyperparameters": {"CAT": {"iterations": 100}}}
        }
    )
    ```

    If you provide `tabular_hyperparameters` with a single model in v1.4.0, a warning will be logged and the parameter will be automatically converted to match the new API.

    If you provide `tabular_hyperparameters` with >=2 models in v1.4.0, an error will be raised since it cannot automatically be converted to the new API.

    </details>

- `Chronos` model: Hyperparameter `optimization_strategy` (deprecated in v1.3.0) has been removed in v1.4.0.

### New Features
- Add `PerStepTabular` model that fits a separate tabular regression model for each step in the forecast horizon. [@shchur] (#5189, #5213)
- Improve heuristic for long-term forecast unrolling (`prediction_length > 64`) for Chronos-Bolt. [@abdulfatir] (#5177)
- `RecursiveTabular` model now supports the `lag_transforms` hyperparameter. [@shchur] (#5184)

### Fixes and Improvements
- Improve the runtime of various `TimeSeriesDataFrame` operations by replacing `groupby` with efficient alternatives based on `indptr`. [@shchur] (#5159)
- Refactor `DirectTabular` and `RecursiveTabular` models to use a single tabular model under the hood instead of a `TabularPredictor`. (#5212)
- Reorganize `autogluon.timeseries.models.gluonts` namespace. [@canerturkmen] (#5104)
- Log the full stack trace in case of individual model failures during training. [@shchur] (#5178)
- Deprecate the `optimization_strategy` hyperparameter for the Chronos (classic) model. [@shchur] (#5202)
- Fix incompatibility with python 3.9. [@prateekdesai04] (#5220)
- Refactor the implementation of `RecursiveTabular` and `DirectTabular` models. [@shchur] (#5184, #5206)
- Fix typos and layout issues in the documentation. [@shchur] (#5225)
- Fix refit_full failing during ensemble prediction if quantile_levels=[]. [@shchur] (#5242)
--------

## Multimodal

- Change multilingual preset to use FP32 to avoid DeBERTa BFloat16. [@tonyhoo] #5139
- Update NLTK dependency constraint to <3.10 to address CVE-2024-39705. [@tonyhoo] #5147

--------

## Documentation and CI
- Fix Python syntax in CUDA library path detection. [@tonyhoo] #5166
- Upgrade image to use torch 2.7.1. [@tonyhoo] #5168
- Show tabular model aliases in the documentation. [@shchur] #5183
- Fix lint check. [@prateekdesai04] #5192
- Update docs to 1.3.2 by @prateekdesai04 in #5137
- Add time limit conversion to seconds in benchmark config script. [@tonyhoo] #5224

--------

## Contributors

Full Contributor List (ordered by # of commits):

@shchur @Innixma @tonyhoo @prateekdesai04 @FireballDWF @canerturkmen @abdulfatir @rsj123 @xiyuanzh @mwhol


### New Contributors
- @mwhol made their first contribution in #5217
