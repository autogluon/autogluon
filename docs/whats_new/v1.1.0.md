# Version 1.1.0

We are happy to announce the AutoGluon 1.1 release.

TODO

Join the community: [![](https://img.shields.io/discord/1043248669505368144?logo=discord&style=flat)](https://discord.gg/wjUmjqAc2N)  
Get the latest updates: [![Twitter](https://img.shields.io/twitter/follow/autogluon?style=social)](https://twitter.com/autogluon)

## Spotlight

### AutoGluon usage on Kaggle

TODO

### Chronos, a foundation model for Time Series forecasting

TODO

## General

- Refactor project README @Innixma (#3861)
- Add AWESOME.md competition results and other doc improvements. @Innixma (#4023)
- Pandas version upgrade. @Innixma (#4044)
- PyTorch, CUDA, Lightning version upgrades. @prateekdesai04 @canerturkmen @zhiqiangdon (#3982, #3984, #3991, #4006)
- Ray version upgrade. @ddelange @tonyhoo (#3774, #3956)
- Scikit-learn version upgrade. @prateekdesai04 (#3872, #3881, #3947)
- Various dependency upgrades. @Innixma (#4024)

## TimeSeries

### Highlights

TODO

### New Features
- Add Chronos Pretrained Forecasting Model. @canerturkmen @shchur @lostella (#3978, #4013, #4052, #4055, #4056, #4061)
- Add native missing value support. @shchur (#3995)
- Add support for categorical covariates. @shchur (#3874, #4037)
- Add feature importance to TimeSeriesPredictor. @canerturkmen (#4033)
- Add .persist logic to TimeSeriesPredictor. @canerturkmen (#4005)
- Add method for plotting forecasts. @shchur (#3889)
- Add RMSLE to metrics @canerturkmen (#3938)
- Add `keep_lightning_logs` hyperparameter. @shchur (#3937)
- Enable logging to file. @canerturkmen (#3877)

### Fixes and Improvements
- Preprocess real-valued features for neural network models (#4042)
- Add option to skip model selection when only one model is trained. @shchur (#4002)
- Ensure all metrics handle missing values in target @shchur (#3966)
- Fix bug when loading a GPU trained model on a CPU machine @shchur (#3979)
- Fix inconsistent random seed. @canerturkmen (#3934)
- Fix crash when calling .info after load. @afmkt (#3900)
- Fix leaderboard crash when no models trained. @shchur (#3849)
- Add prototype TabRepo simulation artifact generation. @shchur (#3829)
- Fix refit_full bug. @shchur (#3820)
- Documentation Improvements, hide deprecated methods. @shchur (#3764, #4054) 
- Minor fixes. @canerturkmen, @shchur, @AnirudhDagar (#4009, #4040, $4041, #4051)

## AutoMM

### Highlights

AutoMM 1.1 introduces the innovative Conv-LoRA, 
a parameter-efficient fine-tuning (PEFT) method stemming from our latest paper presented at ICLR 2024, 
titled "[Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model](https://arxiv.org/abs/2401.17868)". 
Conv-LoRA is designed for fine-tuning the Segment Anything Model, 
exhibiting superior performance compared to previous PEFT approaches, 
such as LoRA and visual prompt tuning, 
across various semantic segmentation tasks in diverse domains 
including natural images, agriculture, remote sensing, and healthcare.

### New Features

- Added Conv-LoRA, a new parameter efficient fine-tuning method. @Harry-zzh @zhiqiangdon (#3933, #3999, #4007, #4022, #4025)
- Added support for new column type: 'image_base64_str'. @Harry-zzh @zhiqiangdon (#3867)
- Added support for loading pre-trained weights in FT-Transformer. @taoyang1122 @zhiqiangdon (#3859)

### Fixes and Improvements

- Fixed bugs in semantic segmentation. @Harry-zzh (#3801, #3812)
- Fixed crashes when using F1 metric. @suzhoum (#3822)
- Fixed bugs in PEFT methods. @Harry-zzh (#3840)
- Accelerated object detection training by ~30\% for the high_quality and best_quality presets. @FANGAreNotGnu (#3970)
- Depreciated Grounding-DINO @FANGAreNotGnu (#3974)
- Fixed lightning upgrade issues @zhiqiangdon (#3991)


## Tabular

### Highlights
AutoGluon-Tabular 1.1 primarily focuses on bug fixes and stability improvements. In particular, we have greatly improved the runtime performance for large datasets between 5 - 30 GB in size through the usage of subsampling for decision threshold calibration and the weighted ensemble fitting to 1 million rows, maintaining the same quality while being far faster to execute. We also adjusted the default weighted ensemble iterations from 100 to 25, which will speedup all weighted ensemble fit times by 4x. We heavily refactored the `fit_pseudolabel` logic, and it should now achieve noticeably stronger results.

### Fixes and Improvements
- Fix return value in `predictor.fit_weighted_ensemble(refit_full=True)`. @Innixma (#1956)
- Enhance performance on large datasets through subsampling. @Innixma (#3977)
- Fix refit_full crash when out of memory. @Innixma (#3977)
- Refactor and enhance `.fit_pseudolabel` logic. @Innixma (#3930)
- Fix crash in memory check during HPO for LightGBM, CatBoost, and XGBoost. @Innixma (#3931)
- Fix dynamic stacking on windows. @Innixma (#3893)
- LightGBM version upgrade. @mglowacki100, @Innixma (#3427)
- Fix memory-safe sub-fits being skipped if Ray is not initialized. @LennartPurucker (#3868)
- Logging improvements. @AnirudhDagar (#3873)
- Hide deprecated methods. @Innixma (#3795)
- Documentation improvements. @Innixma @AnirudhDagar (#2024, #3975, #3976, #3996)

## Docs and CI
- Add auto benchmarking report generation. @prateekdesai04 (#4038, #4039)
- Fix tabular tests for Windows. @tonyhoo (#4036)
- Fix hanging tabular unit tests. @prateekdesai04 (#4031)
- Fix CI evaluation. @suzhoum (#4019)
- Add package version comparison between CI runs @prateekdesai04 (#3962, #3968, #3972)
- Update conf.py to reflect current year. @dassaswat (#3932)
- Avoid redundant unit test runs. @prateekdesai04 (#3942)
- Fix colab notebook links @prateekdesai04 (#3926)
