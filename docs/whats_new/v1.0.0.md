# Version 1.0.0

## General

### Highlights
* Python 3.11 Support @ddelange @yinweisu (#3190)

### Other Enhancements
* Added system info logging utility @Innixma (#3718)

### Dependency Updates
* Upgraded torch to `>=2.0,<2.2` @zhiqiangdon @yinweisu @shchur (#3404, #3587, #3588)
* Upgraded numpy to `>=1.21,<1.29` @prateekdesai04 (#3709)
* Upgraded Pandas to `>=2.0,<2.2` @yinweisu @tonyhoo @shchur (#3498)
* Upgraded scikit-learn to `>=1.3,<1.5` @yinweisu @tonyhoo @shchur (#3498)
* Upgraded Pillow to `>=10.0.1,<11` @jaheba (#3688)
* Upgraded scipy to `>=1.5.4,<1.13` @prateekdesai04 (#3709)
* Upgraded LightGBM to `>=3.3,<4.2` @mglowacki100 @prateekdesai04 @Innixma (#3427, #3709, #3733)
* Various minor dependency updates @jaheba (#3689)

## MultiModal

AutoGluon MultiModal (also known as AutoMM) introduces two new features: 1) Semantic segmentation, and 2) Few shot classification.
Additionally, the presets of PLACEHOLDER are updated and  improve the performance of PLACEHOLDER by PLACEHOLDER with same three lines of codes.

### New Features

* Semantic segmentation
    * Finetuning SAM via LoRA outperforms other Parameter Efficient Finetuning (PEFT) methods like VPT and adaptors on various domain tasks.
    * Customers can finetune SAM for semantic segmentation in a few lines of codes.
    * See tutorials.
    * Contributors and commits: @Harry-zzh  (#3645, #3677, #3697, #3711, #3722), zq (#3728) 
* Few shot classification
    * Few shot classification tasks for both image and text.
    * See tutorials.
    * Contributors and commits: @zhiqiangdon (#3662, #3681) 

### Performance Improvements

* Improving FT-Transformer by PLACEHOLDER on Image only, text+tabular, image+text+tabular tasks. @taoyang1122 (#3732, #3738)

### Other Enhancements

* Add customizing option hf_text.use_fast. Users can choose whether to use fast tokenizer for hf_text models. @zhiqiangdon (#3379) 
* Add fallback evaluation/validation metric. Support f1 macro/micro/weighted. @FANGAreNotGnu (#3696)
* Support custom evaluation metric. @taoyang1122  (#3548)
* Support multi GPU inference with DDP strategy. @zhiqiangdon (#3445, #3451)
* Support multi GPU training in notebook. @zhiqiangdon (#3484)
* Upgrade torch to 2.0. @zhiqiangdon (#3404)
* Upgrade lightning to 2.0 @zhiqiangdon (#3419)
* Upgrade torchmetrics to 1.0 @zhiqiangdon (#3422)

### Deprecations

* Deprecate the config arg in fit API. @zhiqiangdon (#3679)
* Deprecate the init_scratch and pipeline arg in init API. @zhiqiangdon (#3668)

### Code Improvements

* Refactor AutoMM with learner class. @zhiqiangdon (#3650, #3685, #3735)
* Refactor FT-Transformer. @taoyang1122  (#3621, #3700)
* Other code refactor/clean-up: @zhiqiangdon @FANGAreNotGnu (#3383 #3399 #3434 #3667 #3684 #3695)

### Bug Fixes/Doc Improvements

* AutoMM introduction @zhiqiangdon (#3388 #3726)
* Fix multi GPU errors. @FANGAreNotGnu (#3617 #3665 #3684 #3691)
* Other bug fixes @zhiqiangdon @FANGAreNotGnu  @taoyang1122 @tonyhoo  @rsj123 (#3384, #3424, #3526, #3593, #3615, #3638, #3674, #3693, #3702, #3690, #3729, #3736)
* Other doc improvements @zhiqiangdon @FANGAreNotGnu @taoyang1122 (#3397, #3461, #3579, #3670, #3699, #3710, #3716, #3737, #3744, #3745)

## Tabular

### Highlights
AutoGluon v1.0 features major enhancements to predictive quality, establishing a new state-of-the-art in Tabular modeling. The enhancements come primarily from two features: Dynamic stacking to mitigate stacked overfitting, and a new learned model hyperparameters portfolio via Zeroshot-HPO, obtained via the newly released [TabRepo](https://github.com/autogluon/tabrepo) ensemble simulation library. Together, they lead to a **72% win-rate compared to v0.8.2 with faster inference speed, lower disk usage, and higher stability.**

### New Features
* Added `dynamic_stacking` predictor fit argument to mitigate stacked overfitting @LennartPurucker @Innixma (#3616)
* Added zeroshot-HPO learned portfolio as new hyperparameters for `best_quality` and `high_quality` presets. @Innixma @geoalgo (#3750)
* Added `predictor.model_failures()` @Innixma (#3421)
* Added enhanced FT-Transformer @taoyang1122 @Innixma (#3621, #3644, #3692)
* Added `predictor.simulation_artifact()` to support integration with [TabRepo](https://github.com/autogluon/tabrepo) @Innixma (#3555)

### Performance Improvements
* Enhanced FastAI model quality on regression via output clipping @LennartPurucker @Innixma (#3597)
* Added Skip-connection Weighted Ensemble @LennartPurucker (#3598)
* Fix memory leaks by using ray processes for sequential fitting @LennartPurucker (#3614)
* Added dynamic parallel folds support to better utilize compute in low memory scenarios @yinweisu @Innixma (#3511)
* Fixed linear model crashes during HPO and added search space for linear models @Innixma (#3571, #3720)

### Other Enhancements
* Multi-layer stacking now produces deterministic results @LennartPurucker (#3573)
* Various model dependency updates @mglowacki100 (#3373)
* Various code cleanup and logging improvements @Innixma (#3408, #3570, #3652, #3734)

### Bug Fixes / Code and Doc Improvements
* Fixed incorrect model memory usage calculation @Innixma (#3591)
* Fixed `infer_limit` being used incorrectly when bagging @Innixma (#3467)
* Fixed rare edge-case FastAI model crash @Innixma (#3416)
* Various minor bug fixes @Innixma (#3418, #3480)

## TimeSeries

### New features
- Support for custom forecasting metrics @shchur (#3602)
- New forecasting metrics `WAPE`, `RMSSE`, `SQL` + improved documentation for metrics @melopeo @shchur (#3747, #3632, #3510, #3490)
- Improved robustness: `TimeSeriesPredictor` can now handle data with all [pandas frequencies](https://pandas.pydata.org/docs/user_guide/timeseries.html#offset-aliases), irregular timestamps, or missing values represented by `NaN` @shchur (#3563, #3454)
- New models: intermittent demand forecasting models based on conformal prediction (`ADIDA`, `CrostonClassic`, `CrostonOptimized`, `CrostonSBA`, `IMAPA`); `WaveNet` and `NPTS` from GluonTS; new baseline models (`Average`, `SeasonalAverage`, `Zero`)  @canerturkmen @shchur (#3706, #3742, #3606, #3459)
- Advanced cross-validation options: avoid retraining the models for each validation window with `refit_every_n_windows` or adjust the step size between validation windows with `val_step_size` arguments to `TimeSeriesPredictor.fit` @shchur (#3704, #3537)

### Enhancements
- Enable Ray Tune for deep-learning forecasting models @canerturkmen (#3705)
- Support passing multiple evaluation metrics to `TimeSeriesPredictor.evaluate` @shchur (#3646)
- Static features can now be passed directly to `TimeSeriesDataFrame.from_path` and `TimeSeriesDataFrame.from_data_frame` constructors @shchur (#3635)

### Performance improvements
- Much more accurate forecasts at low time limits thanks to new presets and updated logic for splitting the training time across models  @shchur (#3749, #3657, #3741)
- Faster training and prediction + lower memory usage for `DirectTabular` and `RecursiveTabular` models (#3740, #3620, #3559)
- Enable early stopping and improve inference speed for GluonTS models @shchur (#3575)
- Reduce import time for `autogluon.timeseries` by moving import statements inside model classes (#3514)

### Bug Fixes / Code and Doc Improvements
- Improve log messages @shchur (#3721)
- Add reference to the publication on AutoGluon-TimeSeries to README @shchur (#3482)
- Align API of `TimeSeriesPredictor` with `TabularPredictor`, remove deprecated methods @shchur (#3714, #3655, #3396)
- General bug fixes and improvements @shchur(#3746, #3743, #3727, #3698, #3654, #3653, #3648, #3628, #3588, #3560, #3558, #3536, #3533, #3523, #3522, #3476, #3463)

## Deprecations

### General
* `autogluon.core.spaces` has been deprecated. Please use `autogluon.common.spaces` instead @Innixma (#3701)

### Tabular
@Innixma (#3701)
* `autogluon.tabular.TabularPredictor`
  * `predictor.get_model_names()` -> `predictor.model_names()`
  * `predictor.get_model_names_persisted()` -> `predictor.model_names(persisted=True)`
  * `predictor.compile_models()` -> `predictor.compile()`
  * `predictor.persist_models()` -> `predictor.persist()`
  * `predictor.unpersist_models()` -> `predictor.unpersist()`
  * `predictor.get_model_best()` -> `predictor.model_best`
  * `predictor.get_pred_from_proba()` -> `predictor.predict_from_proba()`
  * `predictor.get_oof_pred_proba()` -> `predictor.predict_proba_oof()`
  * `predictor.get_oof_pred()` -> `predictor.predict_oof()`
  * `predictor.get_model_full_dict()` -> `predictor.model_refit_map()`
  * `predictor.get_size_disk()` -> `predictor.disk_usage()`
  * `predictor.get_size_disk_per_file()` -> `predictor.disk_usage_per_file()`
  * `predictor.leaderboard()` `silent` argument deprecated, replaced by `display`, defaults to False
    * Same for `predictor.evaluate()` and `predictor.evaluate_predictions()`

### TimeSeries
* `autogluon.timeseries.TimeSeriesPredictor`
  * Deprecated argument `TimeSeriesPredictor(ignore_time_index: bool)`. Now, if the data contains irregular timestamps, either convert it to regular frequency with `data = data.convert_frequency(freq)` or provide frequenc when creating the predictor as `TimeSeriesPredictor(freq=freq)`.
  * `predictor.evaluate()` now returns a dictionary (previously returned a float)
  * `predictor.score()` -> `predictor.evaluate()`
  * `predictor.get_model_names()` -> `predictor.model_names()`
  * `predictor.get_model_best()` -> `predictor.model_best`
  * Metric `"mean_wQuantileLoss"` has been renamed to `"WQL"`
  * `predictor.leaderboard()` `silent` argument deprecated, replaced by `display`, defaults to False
* `autogluon.timeseries.TimeSeriesDataFrame`
  - `df.to_regular_index()` -> `df.convert_frequency()`
  - Deprecated method `df.get_reindexed_view()`. Please see deprecation notes for `ignore_time_index` under `TimeSeriesPredictor` above for information on how to deal with irregular timestamps
- Models
  - All models based on MXNet (`DeepARMXNet`, `MQCNNMXNet`, `MQRNNMXNet`, `SimpleFeedForwardMXNet`, `TemporalFusionTransformerMXNet`, `TransformerMXNet`) have been removed 
  - Statistical models from Statmodels (`ARIMA`, `Theta`, `ETS`) have been replaced by their counterparts from StatsForecast (#3513). Note that these models now have different hyperparameter names.
  - `DirectTabular` is now implemented using `mlforecast` backend (same as `RecursiveTabular`), most hyperparameter names for the model have changed.
- `autogluon.timeseries.TimeSeriesEvaluator` has been deprecated. Please use metrics available in `autogluon.timeseries.metrics` instead.
- `autogluon.timeseries.splitter.MultiWindowSplitter` and `autogluon.timeseries.splitter.LastWindowSplitter` have been deprecated. Please use `num_val_windows` and `val_step_size` arguments to `TimeSeriesPredictor.fit` instead (alternatively, use `autogluon.timeseries.splitter.ExpandingWindowSplitter`).
