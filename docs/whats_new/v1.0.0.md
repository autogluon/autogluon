# Version 1.0.0

## General

### Highlights
* Python 3.11 Support @ddelange @yinweisu (#3190)

### Other Enhancements
* Added system info logging utility @Innixma (#3718)

### Dependency Updates
* Upgraded torch to `>=2.0,<2.2` @zhiqiangdon @yinweisu @shchur (#3404, #3587, #3588)
* Upgraded numpy to `>=1.21,<1.29` @prateekdesai04 (#3709)
* Upgraded Pandas to `>=2.0,<2.2` @yinweisu @tonyhoo @shchur (#3498)
* Upgraded scikit-learn to `>=1.3,<1.5` @yinweisu @tonyhoo @shchur (#3498)
* Upgraded Pillow to `>=10.0.1,<11` @jaheba (#3688)
* Upgraded scipy to `>=1.5.4,<1.13` @prateekdesai04 (#3709)
* Upgraded LightGBM to `>=3.3,<4.2` @mglowacki100 @prateekdesai04 @Innixma (#3427, #3709, #3733)
* Various minor dependency updates @jaheba (#3689)

## AutoMM

[AutoGluon MultiModal (AutoMM)](https://auto.gluon.ai/dev/tutorials/multimodal/index.html) aims to help users finetune [foundation models](https://en.wikipedia.org/wiki/Foundation_models) on domain applications with three lines of code.
AutoMM supports popular model zoos such as [huggingface/transformers](https://github.com/huggingface/transformers),
[TIMM](https://github.com/huggingface/pytorch-image-models), and [MMDetection](https://github.com/open-mmlab/mmdetection), 
as well as an array of data modalities such as image, text, tabular, and document data, used individually or in combination. 
In v1.0, we introduce several new features such as semantic segmentation, few shot classification, custom evaluation metric,
torch.compile support, and multi-gpu notebook training support. 
Moreover, we upgraded the default image and tabular backbones, 
enhancing the benchmarking performance on image only and text+tabular datasets.


### New Features

* Semantic Segmentation
    * Added the new problem type `semantic_segmentation`, 
      through which users can finetune [Segment Anything Model (SAM)](https://segment-anything.com/)
      on their domain-specific data with three lines of code.
    * Under the hood, we use parameter-efficient finetuning (PEFT) [LoRA](https://arxiv.org/abs/2106.09685)
      to finetune SAM.
    * We have conducted an extensive benchmark encompassing diverse domains, including natural images, agriculture, remote sensing, and healthcare.
      LoRA consistently exhibits superior performance over other PEFT methods such as [VPT](https://arxiv.org/abs/2203.12119), 
      [adaptor](https://arxiv.org/abs/1902.00751), [BitFit](https://arxiv.org/abs/2106.10199),
      [SAM-adaptor](https://arxiv.org/abs/2304.09148), and [LST](https://arxiv.org/abs/2206.06522).
    * Added one [semantic segmentation tutorial](https://auto.gluon.ai/stable/tutorials/multimodal/image_segmentation/beginner_semantic_seg.html).
    * By default, we use [SAM-ViT Huge](https://huggingface.co/facebook/sam-vit-huge), which requires GPU memory > 25GB for LoRA finetuning.
    * Contributors and commits: @Harry-zzh  (#3645, #3677, #3697, #3711, #3722), zq (#3728) 
* Few Shot Classification
    * Added the new problem type `few_shot_classification`, 
      through which users can train few shot classifiers on either images or texts.
    * Under the hood, we use image/text foundation models to extract features 
      and train [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
      classifiers on top of the extracted features.
    * Added one [few shot classification tutorial](https://auto.gluon.ai/stable/tutorials/multimodal/advanced_topics/few_shot_learning.html).
    * Contributors and commits: @zhiqiangdon (#3662, #3681)
* Supported custom evaluation metric. Now users can define their own 
  [metric object](https://auto.gluon.ai/dev/tutorials/tabular/advanced/tabular-custom-metric.html) and pass it to the `eval_metric` argument. @taoyang1122  (#3548)
* Supported multi-gpu training in notebook (experimental). @zhiqiangdon (#3484)
* Supported [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for faster training (experimental).
  * Users can turn it on by setting hyperparameter `env.compile.turn_on` to `True`.
  * Note that users need to upgrade torch >= 2.2 to use this feature.
  * Compiling the model can some time at the beginning. It is recommended for large models or long-time training.
  * It doesn't work for muti-gpu training yet.

### Performance Improvements

* Improved the default image backbone, resulting in a xxx win-rate on the image classification benchmark.
* Replaced MLPs with FT-Transformer as the default tabular backbones, leading to the win-rate xxx on the text+tabular benchmark.
* Improved FT-Transformer by PLACEHOLDER on the tabular benchmark. @taoyang1122 (#3732, #3738)

### Other Enhancements

* Added customizing option hf_text.use_fast. Users can choose whether to use fast tokenizer for hf_text models. @zhiqiangdon (#3379) 
* Added fallback evaluation/validation metric. Support f1 macro/micro/weighted. @FANGAreNotGnu (#3696)

* Supported multi GPU inference with DDP strategy. @zhiqiangdon (#3445, #3451)
* Upgraded torch to 2.0. @zhiqiangdon (#3404)
* Upgraded lightning to 2.0 @zhiqiangdon (#3419)
* Upgraded torchmetrics to 1.0 @zhiqiangdon (#3422)

### Deprecations

* Deprecated the `FewShotSVMPredictor`. Use the new `few_shot_classification` problem type instead.
* Deprecated the `AutoMMPredictor`. Use the `MultiModalPredictor` instead.
* Deprecated the config arg in fit API. @zhiqiangdon (#3679)
* Deprecated the init_scratch and pipeline arg in init API. @zhiqiangdon (#3668)

### Code Improvements

* Refactored AutoMM with learner class. @zhiqiangdon (#3650, #3685, #3735)
* Refactored FT-Transformer. @taoyang1122  (#3621, #3700)
* Other code refactor/clean-up: @zhiqiangdon @FANGAreNotGnu (#3383 #3399 #3434 #3667 #3684 #3695)

### Bug Fixes/Doc Improvements

* AutoMM introduction @zhiqiangdon (#3388 #3726)
* Fixed multi GPU errors. @FANGAreNotGnu (#3617 #3665 #3684 #3691)
* Other bug fixes @zhiqiangdon @FANGAreNotGnu  @taoyang1122 @tonyhoo  @rsj123 (#3384, #3424, #3526, #3593, #3615, #3638, #3674, #3693, #3702, #3690, #3729, #3736)
* Other doc improvements @zhiqiangdon @FANGAreNotGnu @taoyang1122 (#3397, #3461, #3579, #3670, #3699, #3710, #3716, #3737, #3744, #3745)

## Tabular

### Highlights
AutoGluon v1.0 features major enhancements to predictive quality, establishing a new state-of-the-art in Tabular modeling. The enhancements come primarily from two features: Dynamic stacking to mitigate stacked overfitting, and a new learned model hyperparameters portfolio via Zeroshot-HPO, obtained via the newly released [TabRepo](https://github.com/autogluon/tabrepo) ensemble simulation library. Together, they lead to a **72% win-rate compared to v0.8.2 with faster inference speed, lower disk usage, and higher stability.**

### New Features
* Added `dynamic_stacking` predictor fit argument to mitigate stacked overfitting @LennartPurucker @Innixma (#3616)
* Added zeroshot-HPO learned portfolio as new hyperparameters for `best_quality` and `high_quality` presets. @Innixma @geoalgo (#3750)
* Added `predictor.model_failures()` @Innixma (#3421)
* Added enhanced FT-Transformer @taoyang1122 @Innixma (#3621, #3644, #3692)
* Added `predictor.simulation_artifact()` to support integration with [TabRepo](https://github.com/autogluon/tabrepo) @Innixma (#3555)

### Performance Improvements
* Enhanced FastAI model quality on regression via output clipping @LennartPurucker @Innixma (#3597)
* Added Skip-connection Weighted Ensemble @LennartPurucker (#3598)
* Fix memory leaks by using ray processes for sequential fitting @LennartPurucker (#3614)
* Added dynamic parallel folds support to better utilize compute in low memory scenarios @yinweisu @Innixma (#3511)
* Fixed linear model crashes during HPO and added search space for linear models @Innixma (#3571, #3720)

### Other Enhancements
* Multi-layer stacking now produces deterministic results @LennartPurucker (#3573)
* Various model dependency updates @mglowacki100 (#3373)
* Various code cleanup and logging improvements @Innixma (#3408, #3570, #3652, #3734)

### Bug Fixes / Code and Doc Improvements
* Fixed incorrect model memory usage calculation @Innixma (#3591)
* Fixed `infer_limit` being used incorrectly when bagging @Innixma (#3467)
* Fixed rare edge-case FastAI model crash @Innixma (#3416)
* Various minor bug fixes @Innixma (#3418, #3480)

## TimeSeries

### New features
- Support for custom forecasting metrics @shchur (#3602)
- New forecasting metrics `WAPE`, `RMSSE`, `SQL` + improved documentation for metrics @melopeo @shchur (#3747, #3632, #3510, #3490)
- Improved robustness: `TimeSeriesPredictor` can now handle data with all [pandas frequencies](https://pandas.pydata.org/docs/user_guide/timeseries.html#offset-aliases), irregular timestamps, or missing values represented by `NaN` @shchur (#3563, #3454)
- New models: intermittent demand forecasting models based on conformal prediction (`ADIDA`, `CrostonClassic`, `CrostonOptimized`, `CrostonSBA`, `IMAPA`); `WaveNet` and `NPTS` from GluonTS; new baseline models (`Average`, `SeasonalAverage`, `Zero`)  @canerturkmen @shchur (#3706, #3742, #3606, #3459)
- Advanced cross-validation options: avoid retraining the models for each validation window with `refit_every_n_windows` or adjust the step size between validation windows with `val_step_size` arguments to `TimeSeriesPredictor.fit` @shchur (#3704, #3537)

### Enhancements
- Enable Ray Tune for deep-learning forecasting models @canerturkmen (#3705)
- Support passing multiple evaluation metrics to `TimeSeriesPredictor.evaluate` @shchur (#3646)
- Static features can now be passed directly to `TimeSeriesDataFrame.from_path` and `TimeSeriesDataFrame.from_data_frame` constructors @shchur (#3635)

### Performance improvements
- Much more accurate forecasts at low time limits thanks to new presets and updated logic for splitting the training time across models  @shchur (#3749, #3657, #3741)
- Faster training and prediction + lower memory usage for `DirectTabular` and `RecursiveTabular` models (#3740, #3620, #3559)
- Enable early stopping and improve inference speed for GluonTS models @shchur (#3575)
- Reduce import time for `autogluon.timeseries` by moving import statements inside model classes (#3514)

### Bug Fixes / Code and Doc Improvements
- Improve log messages @shchur (#3721)
- Add reference to the publication on AutoGluon-TimeSeries to README @shchur (#3482)
- Align API of `TimeSeriesPredictor` with `TabularPredictor`, remove deprecated methods @shchur (#3714, #3655, #3396)
- General bug fixes and improvements @shchur(#3746, #3743, #3727, #3698, #3654, #3653, #3648, #3628, #3588, #3560, #3558, #3536, #3533, #3523, #3522, #3476, #3463)

## Deprecations

### General
* `autogluon.core.spaces` has been deprecated. Please use `autogluon.common.spaces` instead @Innixma (#3701)

### Tabular
@Innixma (#3701)
* `autogluon.tabular.TabularPredictor`
  * `predictor.get_model_names()` -> `predictor.model_names()`
  * `predictor.get_model_names_persisted()` -> `predictor.model_names(persisted=True)`
  * `predictor.compile_models()` -> `predictor.compile()`
  * `predictor.persist_models()` -> `predictor.persist()`
  * `predictor.unpersist_models()` -> `predictor.unpersist()`
  * `predictor.get_model_best()` -> `predictor.model_best`
  * `predictor.get_pred_from_proba()` -> `predictor.predict_from_proba()`
  * `predictor.get_oof_pred_proba()` -> `predictor.predict_proba_oof()`
  * `predictor.get_oof_pred()` -> `predictor.predict_oof()`
  * `predictor.get_model_full_dict()` -> `predictor.model_refit_map()`
  * `predictor.get_size_disk()` -> `predictor.disk_usage()`
  * `predictor.get_size_disk_per_file()` -> `predictor.disk_usage_per_file()`
  * `predictor.leaderboard()` `silent` argument deprecated, replaced by `display`, defaults to False
    * Same for `predictor.evaluate()` and `predictor.evaluate_predictions()`

### TimeSeries
* `autogluon.timeseries.TimeSeriesPredictor`
  * Deprecated argument `TimeSeriesPredictor(ignore_time_index: bool)`. Now, if the data contains irregular timestamps, either convert it to regular frequency with `data = data.convert_frequency(freq)` or provide frequenc when creating the predictor as `TimeSeriesPredictor(freq=freq)`.
  * `predictor.evaluate()` now returns a dictionary (previously returned a float)
  * `predictor.score()` -> `predictor.evaluate()`
  * `predictor.get_model_names()` -> `predictor.model_names()`
  * `predictor.get_model_best()` -> `predictor.model_best`
  * Metric `"mean_wQuantileLoss"` has been renamed to `"WQL"`
  * `predictor.leaderboard()` `silent` argument deprecated, replaced by `display`, defaults to False
* `autogluon.timeseries.TimeSeriesDataFrame`
  - `df.to_regular_index()` -> `df.convert_frequency()`
  - Deprecated method `df.get_reindexed_view()`. Please see deprecation notes for `ignore_time_index` under `TimeSeriesPredictor` above for information on how to deal with irregular timestamps
- Models
  - All models based on MXNet (`DeepARMXNet`, `MQCNNMXNet`, `MQRNNMXNet`, `SimpleFeedForwardMXNet`, `TemporalFusionTransformerMXNet`, `TransformerMXNet`) have been removed 
  - Statistical models from Statmodels (`ARIMA`, `Theta`, `ETS`) have been replaced by their counterparts from StatsForecast (#3513). Note that these models now have different hyperparameter names.
  - `DirectTabular` is now implemented using `mlforecast` backend (same as `RecursiveTabular`), most hyperparameter names for the model have changed.
- `autogluon.timeseries.TimeSeriesEvaluator` has been deprecated. Please use metrics available in `autogluon.timeseries.metrics` instead.
- `autogluon.timeseries.splitter.MultiWindowSplitter` and `autogluon.timeseries.splitter.LastWindowSplitter` have been deprecated. Please use `num_val_windows` and `val_step_size` arguments to `TimeSeriesPredictor.fit` instead (alternatively, use `autogluon.timeseries.splitter.ExpandingWindowSplitter`).
