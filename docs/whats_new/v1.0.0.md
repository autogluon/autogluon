# Version 1.0.0

## General

### Highlights
* Python 3.11 Support @ddelange @yinweisu (#3190)

### Other Enhancements
* Added system info logging utility @Innixma (#3718)

### Dependency Updates
* Upgraded torch to `>=2.0,<2.2` @zhiqiangdon @yinweisu @shchur (#3404, #3587, #3588)
* Upgraded numpy to `>=1.21,<1.29` @prateekdesai04 (#3709)
* Upgraded Pandas to `>=2.0,<2.2` @yinweisu @tonyhoo @shchur (#3498)
* Upgraded scikit-learn to `>=1.3,<1.5` @yinweisu @tonyhoo @shchur (#3498)
* Upgraded Pillow to `>=10.0.1,<11` @jaheba (#3688)
* Upgraded scipy to `>=1.5.4,<1.13` @prateekdesai04 (#3709)
* Upgraded LightGBM to `>=3.3,<4.2` @mglowacki100 @prateekdesai04 @Innixma (#3427, #3709, #3733)
* Various minor dependency updates @jaheba (#3689)

## AutoMM
[AutoGluon MultiModal (AutoMM)](https://auto.gluon.ai/stable/tutorials/multimodal/index.html) is designed to simplify the fine-tuning of foundation models for downstream applications with just three lines of code. 
It seamlessly integrates with popular model zoos such as [HuggingFace Transformers](https://github.com/huggingface/transformers), 
[TIMM](https://github.com/huggingface/pytorch-image-models), and [MMDetection](https://github.com/open-mmlab/mmdetection), 
providing support for a diverse range of data modalities, 
including image, text, tabular, and document data, whether used individually or in combination.


### New Features

* Semantic Segmentation
    * Introducing the new problem type `semantic_segmentation`, 
      for fine-tuning [Segment Anything Model (SAM)](https://segment-anything.com/) with three lines of code. @Harry-zzh @zhiqiangdon  (#3645, #3677, #3697, #3711, #3722, #3728) 
    * Added comprehensive benchmarks from diverse domains, 
      including natural images, agriculture, remote sensing, and healthcare.
    * Utilizing parameter-efficient finetuning (PEFT) [LoRA](https://arxiv.org/abs/2106.09685), showcasing consistent superior performance over alternatives ([VPT](https://arxiv.org/abs/2203.12119), 
      [adaptor](https://arxiv.org/abs/1902.00751), [BitFit](https://arxiv.org/abs/2106.10199),
      [SAM-adaptor](https://arxiv.org/abs/2304.09148), and [LST](https://arxiv.org/abs/2206.06522)) in the extensive benchmarks.
    * Added one [semantic segmentation tutorial](https://auto.gluon.ai/stable/tutorials/multimodal/image_segmentation/beginner_semantic_seg.html) @zhiqiangdon (#3716).
    * Using [SAM-ViT Huge](https://huggingface.co/facebook/sam-vit-huge) by default (GPU memory > 25GB required).
* Few Shot Classification
    * Added the new `few_shot_classification` problem type for training few shot classifiers on images or texts. @zhiqiangdon (#3662, #3681, #3695)
    * Leveraging image/text foundation models to extract features and train SVM classifiers.
    * Added one [few shot classification tutorial](https://auto.gluon.ai/stable/tutorials/multimodal/advanced_topics/few_shot_learning.html). @zhiqiangdon (#3662)
* Supported [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for faster training (experimental and torch >=2.2 required) @zhiqiangdon (#3520).

### Performance Improvements

* Improved default image backbones, achieving a xxx win-rate on the image benchmark. @taoyang1122 (#3738)
* Replaced MLPs with FT-Transformer as the default tabular backbones, resulting in a xxx win-rate on the text+tabular benchmark. @taoyang1122 (#3732)
* Enhanced FT-Transformer with a xxx win-rate on the tabular benchmark. @taoyang1122 (#3732)

### Stability Enhancements
* Enabled rigorous multi-GPU CI testing. @prateekdesai04 (#3566)
* Fixed multi-GPU issues. @FANGAreNotGnu (#3617 #3665 #3684 #3691, #3639, #3618)

### Enhanced Usability
* Supported custom evaluation metrics, which allows defining custom
  [metric object](https://auto.gluon.ai/dev/tutorials/tabular/advanced/tabular-custom-metric.html) and passing it to the `eval_metric` argument. @taoyang1122  (#3548)
* Supported multi-GPU training in notebooks (experimental). @zhiqiangdon (#3484)
* Improved logging with system info. @zhiqiangdon (#3735)

### Improved Scalability
* The introduction of the new learner class design facilitates easier support for new tasks and data modalities within AutoMM, enhancing overall scalability. @zhiqiangdon (#3650, #3685, #3735)

### Other Enhancements

* Added the option `hf_text.use_fast` for customizing fast tokenizer usage in `hf_text` models. @zhiqiangdon (#3379) 
* Added fallback evaluation/validation metric, supporting `f1_macro` `f1_micro`, and `f1_weighted`. @FANGAreNotGnu (#3696)
* Supported multi-GPU inference with the DDP strategy. @zhiqiangdon (#3445, #3451)
* Upgraded torch to 2.0. @zhiqiangdon (#3404)
* Upgraded lightning to 2.0 @zhiqiangdon (#3419)
* Upgraded torchmetrics to 1.0 @zhiqiangdon (#3422)

### Code Improvements

* Refactored AutoMM with the learner class for improved design. @zhiqiangdon (#3650, #3685, #3735)
* Refactored FT-Transformer. @taoyang1122  (#3621, #3700)
* Refactored the visualizers of object detection, semantic segmentation, and NER. @zhiqiangdon (#3716)
* Other code refactor/clean-up: @zhiqiangdon @FANGAreNotGnu (#3383 #3399 #3434 #3667 #3684 #3695)

### Bug Fixes/Doc Improvements

* Fixed HPO for focal loss. @suzhoum (#3739)
* Fixed one ONNX export issue. @AnirudhDagar (#3725)
* Improved AutoMM introduction for clarity. @zhiqiangdon (#3388 #3726)
* Other bug fixes @zhiqiangdon @FANGAreNotGnu @taoyang1122 @tonyhoo @rsj123 @AnirudhDagar (#3384, #3424, #3526, #3593, #3615, #3638, #3674, #3693, #3702, #3690, #3729, #3736, #3474, #3456, #3590, #3660)
* Other doc improvements @zhiqiangdon @FANGAreNotGnu @taoyang1122 (#3397, #3461, #3579, #3670, #3699, #3710, #3716, #3737, #3744, #3745, #3680)

## Tabular

### Highlights
AutoGluon v1.0 features major enhancements to predictive quality, establishing a new state-of-the-art in Tabular modeling. The enhancements come primarily from two features: Dynamic stacking to mitigate stacked overfitting, and a new learned model hyperparameters portfolio via Zeroshot-HPO, obtained via the newly released [TabRepo](https://github.com/autogluon/tabrepo) ensemble simulation library. Together, they lead to a **72% win-rate compared to v0.8.2 with faster inference speed, lower disk usage, and higher stability.**

### New Features
* Added `dynamic_stacking` predictor fit argument to mitigate stacked overfitting @LennartPurucker @Innixma (#3616)
* Added zeroshot-HPO learned portfolio as new hyperparameters for `best_quality` and `high_quality` presets. @Innixma @geoalgo (#3750)
* Added `predictor.model_failures()` @Innixma (#3421)
* Added enhanced FT-Transformer @taoyang1122 @Innixma (#3621, #3644, #3692)
* Added `predictor.simulation_artifact()` to support integration with [TabRepo](https://github.com/autogluon/tabrepo) @Innixma (#3555)

### Performance Improvements
* Enhanced FastAI model quality on regression via output clipping @LennartPurucker @Innixma (#3597)
* Added Skip-connection Weighted Ensemble @LennartPurucker (#3598)
* Fix memory leaks by using ray processes for sequential fitting @LennartPurucker (#3614)
* Added dynamic parallel folds support to better utilize compute in low memory scenarios @yinweisu @Innixma (#3511)
* Fixed linear model crashes during HPO and added search space for linear models @Innixma (#3571, #3720)

### Other Enhancements
* Multi-layer stacking now produces deterministic results @LennartPurucker (#3573)
* Various model dependency updates @mglowacki100 (#3373)
* Various code cleanup and logging improvements @Innixma (#3408, #3570, #3652, #3734)

### Bug Fixes / Code and Doc Improvements
* Fixed incorrect model memory usage calculation @Innixma (#3591)
* Fixed `infer_limit` being used incorrectly when bagging @Innixma (#3467)
* Fixed rare edge-case FastAI model crash @Innixma (#3416)
* Various minor bug fixes @Innixma (#3418, #3480)

## TimeSeries

### New features
- Support for custom forecasting metrics @shchur (#3602)
- New forecasting metrics `WAPE`, `RMSSE`, `SQL` + improved documentation for metrics @melopeo @shchur (#3747, #3632, #3510, #3490)
- Improved robustness: `TimeSeriesPredictor` can now handle data with all [pandas frequencies](https://pandas.pydata.org/docs/user_guide/timeseries.html#offset-aliases), irregular timestamps, or missing values represented by `NaN` @shchur (#3563, #3454)
- New models: intermittent demand forecasting models based on conformal prediction (`ADIDA`, `CrostonClassic`, `CrostonOptimized`, `CrostonSBA`, `IMAPA`); `WaveNet` and `NPTS` from GluonTS; new baseline models (`Average`, `SeasonalAverage`, `Zero`)  @canerturkmen @shchur (#3706, #3742, #3606, #3459)
- Advanced cross-validation options: avoid retraining the models for each validation window with `refit_every_n_windows` or adjust the step size between validation windows with `val_step_size` arguments to `TimeSeriesPredictor.fit` @shchur (#3704, #3537)

### Enhancements
- Enable Ray Tune for deep-learning forecasting models @canerturkmen (#3705)
- Support passing multiple evaluation metrics to `TimeSeriesPredictor.evaluate` @shchur (#3646)
- Static features can now be passed directly to `TimeSeriesDataFrame.from_path` and `TimeSeriesDataFrame.from_data_frame` constructors @shchur (#3635)

### Performance improvements
- Much more accurate forecasts at low time limits thanks to new presets and updated logic for splitting the training time across models  @shchur (#3749, #3657, #3741)
- Faster training and prediction + lower memory usage for `DirectTabular` and `RecursiveTabular` models (#3740, #3620, #3559)
- Enable early stopping and improve inference speed for GluonTS models @shchur (#3575)
- Reduce import time for `autogluon.timeseries` by moving import statements inside model classes (#3514)

### Bug Fixes / Code and Doc Improvements
- Improve log messages @shchur (#3721)
- Add reference to the publication on AutoGluon-TimeSeries to README @shchur (#3482)
- Align API of `TimeSeriesPredictor` with `TabularPredictor`, remove deprecated methods @shchur (#3714, #3655, #3396)
- General bug fixes and improvements @shchur(#3746, #3743, #3727, #3698, #3654, #3653, #3648, #3628, #3588, #3560, #3558, #3536, #3533, #3523, #3522, #3476, #3463)

## Deprecations

### General
* `autogluon.core.spaces` has been deprecated. Please use `autogluon.common.spaces` instead @Innixma (#3701)

### AutoMM

* Deprecated the `FewShotSVMPredictor` in favor of the new `few_shot_classification` problem type. @zhiqiangdon (#3699)
* Deprecated the `AutoMMPredictor` in favor of `MultiModalPredictor`. @zhiqiangdon (#3650)
* `autogluon.multimodal.MultiModalPredictor`
  * Deprecated the `config` argument in the fit API. @zhiqiangdon (#3679)
  * Deprecated the `init_scratch` and `pipeline` arguments in the init API. @zhiqiangdon (#3668)

### Tabular
@Innixma (#3701)
* `autogluon.tabular.TabularPredictor`
  * `predictor.get_model_names()` -> `predictor.model_names()`
  * `predictor.get_model_names_persisted()` -> `predictor.model_names(persisted=True)`
  * `predictor.compile_models()` -> `predictor.compile()`
  * `predictor.persist_models()` -> `predictor.persist()`
  * `predictor.unpersist_models()` -> `predictor.unpersist()`
  * `predictor.get_model_best()` -> `predictor.model_best`
  * `predictor.get_pred_from_proba()` -> `predictor.predict_from_proba()`
  * `predictor.get_oof_pred_proba()` -> `predictor.predict_proba_oof()`
  * `predictor.get_oof_pred()` -> `predictor.predict_oof()`
  * `predictor.get_model_full_dict()` -> `predictor.model_refit_map()`
  * `predictor.get_size_disk()` -> `predictor.disk_usage()`
  * `predictor.get_size_disk_per_file()` -> `predictor.disk_usage_per_file()`
  * `predictor.leaderboard()` `silent` argument deprecated, replaced by `display`, defaults to False
    * Same for `predictor.evaluate()` and `predictor.evaluate_predictions()`

### TimeSeries
* `autogluon.timeseries.TimeSeriesPredictor`
  * Deprecated argument `TimeSeriesPredictor(ignore_time_index: bool)`. Now, if the data contains irregular timestamps, either convert it to regular frequency with `data = data.convert_frequency(freq)` or provide frequenc when creating the predictor as `TimeSeriesPredictor(freq=freq)`.
  * `predictor.evaluate()` now returns a dictionary (previously returned a float)
  * `predictor.score()` -> `predictor.evaluate()`
  * `predictor.get_model_names()` -> `predictor.model_names()`
  * `predictor.get_model_best()` -> `predictor.model_best`
  * Metric `"mean_wQuantileLoss"` has been renamed to `"WQL"`
  * `predictor.leaderboard()` `silent` argument deprecated, replaced by `display`, defaults to False
* `autogluon.timeseries.TimeSeriesDataFrame`
  - `df.to_regular_index()` -> `df.convert_frequency()`
  - Deprecated method `df.get_reindexed_view()`. Please see deprecation notes for `ignore_time_index` under `TimeSeriesPredictor` above for information on how to deal with irregular timestamps
- Models
  - All models based on MXNet (`DeepARMXNet`, `MQCNNMXNet`, `MQRNNMXNet`, `SimpleFeedForwardMXNet`, `TemporalFusionTransformerMXNet`, `TransformerMXNet`) have been removed 
  - Statistical models from Statmodels (`ARIMA`, `Theta`, `ETS`) have been replaced by their counterparts from StatsForecast (#3513). Note that these models now have different hyperparameter names.
  - `DirectTabular` is now implemented using `mlforecast` backend (same as `RecursiveTabular`), most hyperparameter names for the model have changed.
- `autogluon.timeseries.TimeSeriesEvaluator` has been deprecated. Please use metrics available in `autogluon.timeseries.metrics` instead.
- `autogluon.timeseries.splitter.MultiWindowSplitter` and `autogluon.timeseries.splitter.LastWindowSplitter` have been deprecated. Please use `num_val_windows` and `val_step_size` arguments to `TimeSeriesPredictor.fit` instead (alternatively, use `autogluon.timeseries.splitter.ExpandingWindowSplitter`).
