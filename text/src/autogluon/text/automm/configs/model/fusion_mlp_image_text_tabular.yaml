model:
  names:
    - "categorical_mlp"
    - "numerical_mlp"
    - "hf_text"
    - "timm_image"
    - "clip"
    - "fusion_mlp"
  categorical_mlp:
    hidden_size: 64
    activation: "leaky_relu"
    num_layers: 1
    drop_rate: 0.1
    normalization: "layer_norm"
    data_types:
      - "categorical"
  categorical_transformer:
    out_features: 192
    d_token: 192
    num_trans_blocks: 0
    num_attn_heads: 8
    residual_dropout: 0.0
    attention_dropout: 0.2
    ffn_dropout: 0.1
    normalization: layer_norm
    ffn_activation: reglu
    head_activation: relu
    data_types:
    - categorical
  numerical_mlp:
    hidden_size: 128
    activation: "leaky_relu"
    num_layers: 1
    drop_rate: 0.1
    normalization: "layer_norm"
    data_types:
      - "numerical"
    merge: "concat"
  numerical_transformer:
    out_features: 192
    d_token: 192
    num_trans_blocks: 0
    num_attn_heads: 8
    residual_dropout: 0.0
    attention_dropout: 0.2
    ffn_dropout: 0.1
    normalization: "layer_norm"
    ffn_activation: "reglu"
    head_activation: "relu" 
    data_types:
      - "numerical"
    merge: "concat"
  hf_text:
    checkpoint_name: "google/electra-base-discriminator"
    data_types:
      - "text"
    tokenizer_name: "hf_auto"
    max_text_len: 512  # If None or <=0, then use the max length of pretrained models.
    insert_sep: True
    text_segment_num: 2
    stochastic_chunk: False
  timm_image:
    checkpoint_name: "swin_base_patch4_window7_224"
    mix_choice: "all_logits"
    data_types:
      - "image"
    train_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    val_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    image_norm: "imagenet"
    image_size: 224
    max_img_num_per_col: 2
  clip:
    checkpoint_name: "openai/clip-vit-base-patch32"
    data_types:
      - "image"
      - "text"
    train_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    val_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    image_norm: "clip"
    image_size: 224
    max_img_num_per_col: 2
    tokenizer_name: "clip"
    max_text_len: 77  # The maximum possible length.
    insert_sep: False
    text_segment_num: 1
    stochastic_chunk: False
  fusion_mlp:
    weight: 0.1
    adapt_in_features: "max"
    hidden_sizes:
      - 128
    activation: "leaky_relu"
    drop_rate: 0.1
    normalization: "layer_norm"
    data_types:
  fusion_transformer:
    hidden_size: 192
    n_blocks: 3
    attention_n_heads: 8
    adapt_in_features: "max"
    attention_dropout: 0.2
    residual_dropout: 0.0
    ffn_dropout: 0.1 
    ffn_d_hidden: 192 
    normalization: "layer_norm"
    ffn_activation: "geglu"
    head_activation: "relu" 
    data_types:
