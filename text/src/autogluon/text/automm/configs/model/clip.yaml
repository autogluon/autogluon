model:
  names: "clip"
  clip:
    checkpoint_name: "openai/clip-vit-base-patch32"
    features_only: False
    data_types:
      - "image"
      - "text"
    train_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    val_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    image_norm: "clip"
    image_size: 224
    max_img_num_per_col: 0
    tokenizer_name: "clip"
    max_text_len: 77  # The maximum possible length.
    insert_sep: False
    text_segment_num: 1
    stochastic_chunk: False
