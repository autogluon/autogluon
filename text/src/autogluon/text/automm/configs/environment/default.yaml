env:
  num_gpus: -1
  num_nodes: 1
  batch_size: 128  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.
  per_gpu_batch_size: 16  # you can define this according to your GPU memory
  precision: 16  # training precision
  num_workers: 2  # pytorch training dataloader workers
  num_workers_evaluation: 2  # pytorch prediction/test dataloader workers
  fast_dev_run: False
  deterministic: False
  auto_select_gpus: True
  strategy: "ddp"
