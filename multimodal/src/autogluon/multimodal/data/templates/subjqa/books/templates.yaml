dataset: subjqa
subset: books
templates:
  071f2b19-7392-4258-8a60-5a96f3e44b0d: !Template
    answer_choices: null
    id: 071f2b19-7392-4258-8a60-5a96f3e44b0d
    jinja: 'Context:

      {{context}}


      Answer the following question with extracts from the context: {{question}}


      |||

      {% if (answers["text"]  | length) == 0 %}

      {{ "Unanswerable" }}

      {% else %}

      {{answers["text"][0]}}

      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Squad
      original_task: true
    name: answer_q_with_context_first
    reference: Original prompt with the context in the beginning.
  12812357-1dce-4e33-a6a4-a6ccea8cafcf: !Template
    answer_choices: null
    id: 12812357-1dce-4e33-a6a4-a6ccea8cafcf
    jinja: '{{question}}


      Answer using extracts from the following context. If you can''t find an answer,
      return {{"Unanswerable"}}


      Context:

      {{context}}


      Hint: The context domain is {{domain}}


      |||

      {% if (answers["text"]  | length) == 0 %}

      {{ "Unanswerable" }}

      {% else %}

      {{answers["text"][0]}}

      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Squad
      original_task: true
    name: domain_hint_og_task
    reference: Original task template with the domain hint
  69e2f180-3b0f-4a00-b7e3-75f2a572ff06: !Template
    answer_choices: null
    id: 69e2f180-3b0f-4a00-b7e3-75f2a572ff06
    jinja: "In today's exam on {{domain}}, answer the following question with the\
      \ help of the context. If the question cannot be answered, say Unanswerable.\n\
      \nQuestion: \n{{question}}\n\nContext:\n{{context}}\n\n|||\n{% if (answers[\"\
      text\"]  | length) == 0 %}\n{{ \"Unanswerable\" }}\n{% else %}\n{{answers[\"\
      text\"][0]}}\n{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Squad
      original_task: true
    name: exam_style_prompt
    reference: 'Exam style original task prompt '
  82092877-688a-4e56-8617-31b113cc6653: !Template
    answer_choices: null
    id: 82092877-688a-4e56-8617-31b113cc6653
    jinja: "To get full credit in today's test,  answer the following question with\
      \ the help of the context. If the question cannot be answered, say Unanswerable.\n\
      \nQuestion: \n{{question}}\n\nContext:\n{{context}}\n\n|||\n{% if (answers[\"\
      text\"]  | length) == 0 %}\n{{ \"Unanswerable\" }}\n{% else %}\n{{answers[\"\
      text\"] | join(\" \\n \")}}\n{% endif %}"
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Squad
      original_task: true
    name: exam_style_without_hint
    reference: Exam style prompt without hint
  a217525b-caf2-4ae3-8a6e-06bd48bf4728: !Template
    answer_choices: null
    id: a217525b-caf2-4ae3-8a6e-06bd48bf4728
    jinja: '{{question}}


      Answer using extracts from the following context. If you can''t find an answer,
      return {{"Unanswerable"}}


      Context:

      {{context}}


      |||

      {% if (answers["text"]  | length) == 0 %}

      {{ "Unanswerable" }}

      {% else %}

      {{answers["text"][0]}}

      {% endif %}'
    metadata: !TemplateMetadata
      choices_in_prompt: false
      languages:
      - en
      metrics:
      - Squad
      original_task: true
    name: answer_q_with_context_after
    reference: Prompt has instructions to answer to question along with the context
  afe5086e-d9fe-4981-bcac-67d580950110: !Template
    answer_choices: 1 ||| 2 ||| 3 ||| 4 ||| 5
    id: afe5086e-d9fe-4981-bcac-67d580950110
    jinja: 'Question:

      {{question}}


      On a scale of 1 to 5 (1 being the most subjective), how subjective is the question?


      |||


      {{answer_choices[question_subj_level -1]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - en
      metrics:
      - Accuracy
      original_task: false
    name: q_subj_score
    reference: Prompt asks the rate the subjectivity of the question
  b4a015eb-9346-4739-9ebd-5f91d2f230be: !Template
    answer_choices: 1 ||| 2 ||| 3 ||| 4 ||| 5
    id: b4a015eb-9346-4739-9ebd-5f91d2f230be
    jinja: 'Context:

      {{context}}


      Question:

      {{question}}


      How would you rate the subjectivity of the question (on a 1 to 5 scale with
      1 being the most subjective)?


      |||


      {{answer_choices[question_subj_level -1]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - en
      metrics:
      - Accuracy
      original_task: false
    name: q_subj_score_with_context
    reference: The prompt asks to rate the subjectivity of the question
  f074e3ce-966c-4d63-8d03-f0b6e5093b38: !Template
    answer_choices: books|||electronics|||grocery|||movies|||restaurants|||tripadvisor
    id: f074e3ce-966c-4d63-8d03-f0b6e5093b38
    jinja: '{% set mapping = {"books": 0, "electronics": 1, "grocery": 2, "movies":
      3, "restaurants":4 , "tripadvisor": 5} %}

      Possible categories:

      - {{ ["books", "electronics", "grocery", "movies", "restaurants", "tripadvisor"]  |
      join("\n- ") }}


      Context:

      {{context}}


      Which of the category corresponds to the context?


      |||


      {{answer_choices[mapping[domain]]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - en
      metrics:
      - Accuracy
      original_task: false
    name: domain_q
    reference: The prompt asks to pick the category for the context
  f53cc9f9-1d34-47ff-b440-a6ad896bdc4a: !Template
    answer_choices: books ||| electronics ||| grocery ||| movies ||| restaurants |||
      tripadvisor
    id: f53cc9f9-1d34-47ff-b440-a6ad896bdc4a
    jinja: '{% set mapping = {"books": 0, "electronics": 1, "grocery": 2, "movies":
      3, "restaurants":4 , "tripadvisor": 5} %}

      Context:

      {{context}}


      Which of {{"books, electronics, grocery, movies, restaurants or tripadvisor"}}
      corresponds to the context?


      |||


      {{answer_choices[mapping[domain]]}}'
    metadata: !TemplateMetadata
      choices_in_prompt: true
      languages:
      - en
      metrics:
      - Accuracy
      original_task: false
    name: domain_q_after_context
    reference: Another prompt asking to pick the correct category
