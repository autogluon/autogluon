env:
  num_gpus: -1
  num_nodes: 1
  batch_size: 128  # this is a desired batch size; pl trainer will accumulate gradients when per step batch is smaller.
  per_gpu_batch_size: 8  # training per gpu batch size
  eval_batch_size_ratio: 4  # per_gpu_batch_size_evaluation = per_gpu_batch_size * eval_batch_size_ratio
  per_gpu_batch_size_evaluation:  # This is deprecated. Use eval_batch_size_ratio instead.
  precision: 16  # training precision. Refer to https://pytorch-lightning.readthedocs.io/en/latest/advanced/mixed_precision.html
  num_workers: 2  # pytorch training dataloader workers
  num_workers_evaluation: 2  # pytorch prediction/test dataloader workers
  fast_dev_run: False
  deterministic: False
  auto_select_gpus: True
  strategy: "ddp_spawn"
  deepspeed_allgather_size: 1e9 # DeepSpeed allgather size (number of elements gathered at a time). Limits memory required for allgather on large models.
  deepspeed_allreduce_size: 1e9 # DeepSpeed allreduce size (number of elements reduced at a time). Limits memory required for allreduce on large models.
