model:
  names:
    - "categorical_mlp"
    - "numerical_mlp"
    - "hf_text"
    - "timm_image"
    - "clip"
    - "fusion_mlp"
  categorical_mlp:
    hidden_size: 64
    activation: "leaky_relu"
    num_layers: 1
    drop_rate: 0.1
    normalization: "layer_norm"
    data_types:
      - "categorical"
  categorical_transformer:
    out_features: 192
    d_token: 192
    num_trans_blocks: 0
    num_attn_heads: 8
    residual_dropout: 0.0
    attention_dropout: 0.2
    ffn_dropout: 0.1
    normalization: layer_norm
    ffn_activation: reglu
    head_activation: relu
    data_types:
    - categorical
  numerical_mlp:
    hidden_size: 128
    activation: "leaky_relu"
    num_layers: 1
    drop_rate: 0.1
    normalization: "layer_norm"
    d_token: 8
    embedding_arch:
    data_types:
      - "numerical"
    merge: "concat"
  numerical_transformer:
    out_features: 192
    d_token: 192
    num_trans_blocks: 0
    num_attn_heads: 8
    residual_dropout: 0.0
    attention_dropout: 0.2
    ffn_dropout: 0.1
    normalization: "layer_norm"
    ffn_activation: "reglu"
    head_activation: "relu" 
    data_types:
      - "numerical"
    embedding_arch:
      - 'linear'
      - 'relu'
    merge: "concat"
  hf_text:
    checkpoint_name: "google/electra-base-discriminator"
    gradient_checkpointing: False
    pooling_mode: 'cls'  # The pooling mode, can be 'cls' or 'mean'
    data_types:
      - "text"
    tokenizer_name: "hf_auto"
    max_text_len: 512  # If None or <=0, then use the max length of pretrained models.
    insert_sep: True
    text_segment_num: 2
    stochastic_chunk: False
    text_aug_detect_length: 10                # We perform text augmentation only if a text has more than text_detection_length words. It is used to differentiate text columns versus tabular columns that are treated as text.
    text_trivial_aug_maxscale: 0.0            # augmentation magnititude randomly drawn from [0, text_trivial_aug_maxscale]
    text_train_augment_types:        # specify augmentation space manually, will randomly select one from the following and identity
      # - "random_swap(0.05)"          # less than 0.1 based on eda paper
      # - "random_delete(0.05)"        # less than 0.1 based on eda paper
      # - "syn_replacement(0.05)"  # less than 0.1 based on eda paper
      # - "insert_punc(0.05)"

  timm_image:
    checkpoint_name: "swin_base_patch4_window7_224"
    mix_choice: "all_logits"
    data_types:
      - "image"
    train_transform_types:
      - "resize_shorter_side"
      - "center_crop"
      - "trivial_augment"
    val_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    image_norm: "imagenet"
    image_size: 224
    max_img_num_per_col: 2
  clip:
    checkpoint_name: "openai/clip-vit-base-patch32"
    data_types:
      - "image"
      - "text"
    train_transform_types:
      - "resize_shorter_side"
      - "center_crop"
      - "trivial_augment"
    val_transform_types:
      - "resize_shorter_side"
      - "center_crop"
    image_norm: "clip"
    image_size: 224
    max_img_num_per_col: 2
    tokenizer_name: "clip"
    max_text_len: 77  # The maximum possible length.
    insert_sep: False
    text_segment_num: 1
    stochastic_chunk: False
    text_aug_detect_length: 10                     # We perform text augmentation only if a text has more than text_detection_length words. It is used to differentiate text columns versus tabular columns that are treated as text.
    text_trivial_aug_maxscale: 0.0                      # scale randomly drawn from [0, text_trivial_aug_maxscale]
    text_train_augment_types:        # specify augmentation space manually, will randomly select one from the following and identity
      # - "random_swap(0.05)"          # less than 0.1 based on eda paper
      # - "random_delete(0.05)"        # less than 0.1 based on eda paper
      # - "syn_replacement(0.05)"  # less than 0.1 based on eda paper
      # - "insert_punc(0.05)"
  fusion_mlp:
    weight: 0.1
    adapt_in_features: "max"
    hidden_sizes:
      - 128
    activation: "leaky_relu"
    drop_rate: 0.1
    normalization: "layer_norm"
    data_types:
  fusion_transformer:
    hidden_size: 192
    n_blocks: 3
    attention_n_heads: 8
    adapt_in_features: "max"
    attention_dropout: 0.2
    residual_dropout: 0.0
    ffn_dropout: 0.1 
    ffn_d_hidden: 192 
    normalization: "layer_norm"
    ffn_activation: "geglu"
    head_activation: "relu" 
    data_types:
